1. Cloud Storage.
	
	El Cloud Storage, o almacenamiento en la nube, es un modelo de almacenamiento basado en redes donde los datos son almacenados en piscinas virtuales de almacenamiento. 
	En las arquitecturas de almacenamiento en la nube, se persiguen cuatro objetivos, la agilidad, o capacidad de mover los datos al sitio adecuado para mejorar su disponibilidad, la escalabilidad, o capacidad de manejar crecientes cargas de trabajo, o de adaptarse para ser capaces de soportarlas, la elasticidad, o la capacidad de escalarse sin limites razonables, y la capacidad de ser multiusuario.
	El Cloud Storage debe estar formado por servicios distribuidos que deben comportarse como un conjunto, debe ser tolerante a fallos a través de la redundancia de datos, debe ser duradero, sirviéndose de la creación de versiones de los documentos alojados, y debe ser consistente entre las diferentes versiones almacenadas.
	Las principales preocupaciones de dichas soluciones son la seguridad ante posibles accesos indebidos, la estabilidad de la empresa proveedora, la accesibilidad de la información, y los costes hardware de la solución utilizada.
	Actualmente, existen dos tipos de instalaciones de Cloud Storage, las realizadas en servidores externos, y las instalaciones locales que el administrador puede acomodar a sus necesidades. 
	En servidores externos, la compañías que ofrecen este servicio establecen una piscina de servidores virtuales en los que se aloja dicho almacenamiento y los pone a disposición del usuario. Este tipo de instalaciones goza de las mayores ventajas del Cloud Storage, que se apoyan en la disposición de grandes servidores de almacenamiento, y la delegación de la tarea de la administración y responsabilidades a la compañía encargada de ofrecer el servicio.
	En las instalaciones locales, el usuario debe instalar la plataforma de almacenamiento en la nube, adaptarla a sus necesidades, y administrarla adecuadamente con el fin que persigue. Para el objetivo de este proyecto estudiaremos en mayor profundidad las soluciones locales.

2. Almacenamiento en soluciones libres y locales existentes.

2.1 Amazon S3.
	La plataforma Amazon S3 no se trata expresamente de una plataforma libre, ni de instalacin local, puesto que solo se ofrecen las distintas interfaces de comunicación de esta. Aun así es objeto de estudio debido a que algunas soluciones basan su almacenamiento en esta plataforma.
	La Amazon Simple Storage Service tiene como objetivo ser escalable, tener alta disponibilidad y baja latencia, a un bajo coste. Dispone de tres tipos de interfaces distintas, REST(Representational State Transfer), SOAP(Simple Object Access Protocol) y BitTorrent. Las primera interfaz es la más adoptada, se trata de una interfaz para sistemas distribuidos adoptada ampliamente en la World Wide Web. La segunda es una interfaz que cumple el mismo propósito que REST, pero basándose en tecnología XML. La tercera es una de las conocidas tecnologías P2P existentes.
	Admite objetos de un tamaño inferior a 5 terabytes de información y los organiza en cubos. Para cada cubo, y objeto, dentro de este, se dispone una lista de control de acceso que permite al usuario disponer de su información. Cada objeto en un cubo puede ser accedido directamente desde internet. Además, cada objeto es una semilla de la red BitTorrent que puede ser utilizada para reducir el ancho de banda utilizado durante la descarga.
	Puesto que el principal uso de los cubos es almacenar páginas web, cada cubo puede ser configurado para almacenar los registros de acceso en un cubo gemelo, o réplica.
	Esta solución es capaz de alojar maquinas virtuales que se pueden ejecutar a través de sus interfaces expuestas, y admite redundancia distribuida en su cluster de almacenamiento.
	Debido a su condición de no tener instalación local, no es posible saber sobre que tipo de host funciona dicha solución.
	
2.2 OpenNebula.
	Aunque la plataforma de Cloud Computing de OpenNebula es compatible con la interfaz Amazon EC2 Query Interface, lo que le ofrece compatibilidad con la plataforma de almacenamiento Amazon S3, dicha plataforma goza de un subsistema de almacenamiento propio. 
	El subsistema está ideado para almacenar máquinas virtuales únicamente. Dicha plataforma se compone de una serie de drivers divididos en dos tipos, data store drivers, o drivers de almacenamiento de datos, y transfer manager drivers, o drivers de gestión de transferencia. Los primeros se encargan de ofrecer las funcionalidades de creación, eliminación y modificación de las máquinas virtuales. Los segundos se encargan de realizar las transferencias a otras localizaciones, clonación y enlace.
	Los drivers de transferencia proveen a esta solución de una interfaz REST, y no es posible separar los subsistemas de comunicación de los de gestión de almacenamiento y operación de esta nube. No se especifica que disponga de persistencia de datos replicandolos a lo largo del cluster formado por la nube. Los tipos de host donde pueden ser alojada esta solución deben ser servidores Linux.

2.3 Nimbus.
	La plataforma Nimbus utiliza una implementación propia y libre de la API REST(Representational State Transfer) de Amazon S3, llamada Cumulus, para su almacenamiento. Las interfaces que ofrece dicha implementación son de tipo REST y SOAP, y no es posible separar la interfaz de comunicación del almacenamiento subyaciente. Su almacenamiento, al igual que OpenNebula está basado en máquinas virtuales que gestionan sus propios discos, no siendo posible definir un almacenamiento sin sistema operativo. Esta solución permite, sin embargo, realizar replicación distribuida de datos para garantizar la persistencia. Los host que soportan esta solución deben ser servidores Linux.

2.4 OpenStack.
	Dicha solución se divide en tres partes bien diferenciadas para ofrecer una nube personal, estas son Nova, para la ejecución de software, Swift, para el almacenamiento, y Glance, para la gestión de imágenes virtuales. Esto le ofrece la capacidad de poder instalar solo el componente necesario para las necesidades del usuario. Swift es capaz de almacenar objetos o bloques indistintamente, permitiendo no solo el almacenamiento de imágenes virtuales, sino de cualquier tipo de información. 
	Esta solución es escalable, distribuida, admite redundancia de datos, y propone al usuario una API de programación. Dicha API es compatible con las soluciones de almacenamiento masivo NetApp, Nexenta, SolidFire y Amazon S3. Debido a esto, admite comunicaciones a través de una interfaz REST o SOAP. El soporte de bloques permite que cualquier dispositivo de bloques(p. e. un disco duro, una cinta magnética, o una imagen virtual) pueda añadirse al sistema de almacenamiento distribuido de forma transparente.
	Su organización se basa en clusteres de almacenamiento, y cada objeto se distribuye entre diferentes nodos de la red. El sistema garantiza la persistencia de bloques y capacidad de realizar instantaneas(snapshots) del almacenamiento para labores de salvaguardas.
	Una vez más el tipo de servidor que es capaz de alojarlo es un servidor Linux.

2.5 OpenQRM.
	En  el caso de OpenQRM nos encontramos con una solución propia, basada en el sistema de plugins para la plataforma. Dichos plugins se encargan de dar soporte por separado a los distintos tipos de almacenamientos reales en las distintas máquinas. Para ello, proveen al usuario de una interfaz SOAP. Su tipo de almacenamiento es basado en máquinas virtuales y no admite replicación de datos a lo largo del cluster.
	Entre ellos podemos encontrarnos los siguientes plugins, que nos dan una idea del tipo de almacenamiento tratado: netapp-storage, nfs-storage, local-storage, iscsi-storage, lvm-storage, xen-storage y kvm-storage.
	Dicha solución debe ser instalada sobre un servidor Linux.

2.6 Eucalyptus.
	Entre los distintos componentes de esta solución de Cloud Computing se encuentra el Storage Controller, o controlador de almacenamiento. Dicho componente implementa el sistema de almacenamiento Amazon EBS, Elastic Block Store, el cual provee de dispositivos de bloques brutos a la plataforma, siguiendo la estructura detallada en la plataforma Amazon S3. Esta implementación le permite utilizar las interfaces REST, SOAP y Bittorrent para manejar el almacenamiento, y en este caso es posible separar el subsistema de comunicación del de almacenamiento en servidores distintos.
	Los hosts que soportan dicha solución deben ser servidores Linux.

2.7 CloudStack.
	Esta plataforma se nutre de la solución de almacenamiento de OpenStack, Swift, estudiada anteriormente, por tanto, ofrece interfaces REST y SOAP para su comunicación, cualquier tipo de almacenamiento, almacenamiento distribuido a lo largo del cluster, y, a diferencia de OpenStack, es capaz de instalarse sin sistema operativo servidor.

2.8 Abiquo Open source edition.
	En esta plataforma nos encontramos con que el almacenamiento es independiente al resto de la plataforma, ofreciendo solo almacenamiento virtual de dos tipos: asistido, o genérico iSCSI. En el caso del almacenamiento asistido la plataforma ofrece almacenamiento como servicio directamente al usuario. En el caso del almacenamiento genérico iSCSI, el usuario dispone de un almacenamiento preconfigurado por el administrador del sistema, habitualmente una máquina virtual. Para ello se apoya en un conjunto de plugins, al igual que OpenQRM, que dan soporte a los siguientes tipos de almacenamiento: LVM y iSCSI Linux, Nexenta, NetApp. Aparte se ofrece una API de desarrollo para facilitar el soporte de otras plataformas como Dell Equallogic, IBM Volume Manager.
	Esta solución solo ofrece una interfaz REST al usuario, y no ofrece replicación distribuida de forma automática. En este caso no se requiere de ningún sistema operativo para alojar la nube.

3. Comparación de soluciones.

	La mayor parte de las soluciones estudiadas implementan al menos alguna de las interfaces de comunicación propuestas por la nube de Amazon, además de compartir su tipo de almacenamiento, y, al menos, ofrecen al usuario de almacenamiento basado en máquinas virtuales, sin embargo, no todas ofrecen la oportunidad de realizar una instalación minimalista orientada al almacenamiento. Exceptuando la nube de Abiquo, y por razones obvias, la solución de Amazon, todas deben ser alojadas en servidores Linux, no soportándose la instalación nativa sobre un servidor, o la instalación sobre servidores Windows.
	De este amplio abanico de soluciones es necesario destacar el soporte de las tres interfaces habituales por parte de Eucalyptus, y su soporte para distintos tipos de almacenamiento, admitiendo flexibilidad de instalación. Sin embargo, esta poderosa nube está orientada a ofrecer todos los servicios disponibles en cualquier nube habitual(computación, virtualización y almacenamiento), y no es especialmente cómoda para realizar una instalación minimalista, orientada al almacenamiento.
	La solución Nimbus, a través de su servidor Cumulus, ofrece una imagen de sencillez si el objetivo perseguido es configurar una nube de almacenamiento, sin embargo su fuerte orientación a requerir máquinas virtuales con sistemas operativos propios hacen que la solución pierda atractivo. OpenNebula y OpenQRM, disponen de características similares, conformando las soluciones más rígidas de todas, pero a la vez las más sencillas de administrar. Además, su implementación basada en drivers o plugins les ofrece facilidad de desarrollo de código y alta modularidad.
	Las soluciones Openstack y Cloudstack ofrecen alta flexibilidad de instalación, una modularización aceptable, y por tanto, facilidad de mantenimiento de código, y la posibilidad de hacer instalaciones minimalistas orientadas a cada uno de sus servicios por separado. Esta última característica las convierten en plataformas adecuadas para cualquier tipo de desarrollo sobre cualquiera de sus servicios.
	Las características de las distintas plataformas se resumen en la siguiente tabla:

	
Característica / Nube
 
Amazon S3
OpenNebula
Nimbus
OpenStack
OpenQRM
Eucalyptus
Cloudstack
Abiquo O.S.E.
APIs soportadas
REST
Si
Si
Si
Si
No
Si
Si
Si

SOAP
Si 
No
Si
Si
Si
Si
Si
No

Bittorrent
Si
No
No
No
No
Si
No
No
Comunicación independiente de almacenamiento
No
Si
No
Si
No
Si
Si
Si
Tipo almacenamiento
M.V.
Si
Si
Si
Si
Si
Si
Si
Si

Puro
Si
No
No
Si
No
Si
Si
Si
Cluster de replicación
Si
No
Si
Si
No
Si
Si
No
Host
No requerido
?
No
No
No
No
No
Si
Si

Linux
?
Si
Si
Si
Si
Si
Si
No

Windows
?
No
No
No
No
No
No
No

4. Swiftness.

	El objetivo principal de Swiftness es ofrecer drivers de código abierto, para los distintos sistemas operativos existentes, y para la administración de las soluciones de almacenamiento distribuidos basadas en Swift de OpenStack. Swift nos ofrece la posibilidad de disponer del almacenamiento requerido en nuestra propia nube privada, prescindiendo de componentes innecesarios como los de computación en la nube o administración de maquinas virtuales. Al no estar su almacenamiento orientado exclusivamente al alojamiento de máquinas virtuales, Swift se convierte en una solución perfecta para establecer modelo de negocio que quiera mantener datos de forma segura en la red, pudiéndose plantear un modelo de negocio basado en la instalación y mantenimiento de nubes privadas, de almacenamiento o de computación. Swiftness permitirá acelerar cualquier tarea relacionada con el almacenamiento permitiendo mejorar el margen de beneficios en dichos servicios.
	Para ello Swiftness propone una capa de abstracción de software que se interpondrá entre el núcleo del sistema operativo y el sistema de ficheros del cliente.

5. Swift.

	Este componente de la nube propuesta por Openstack se compone de cuatro servidores que se comunican para ofrecer el servicio propuesto. Estos son, un servidor de cuentas, un servidor de contenedores, un servidor de objetos y un servidor proxy para distribuir las peticiones entre el resto de servidores. De esta forma, para realizar cualquier transacción, el cliente deberá solicitar, al servidor de cuentas, acceso a la plataforma. Validado su acceso, dispondrá de acceso al contenedor asociado a su cuenta, y a los objetos asociados a este contenedor, cada uno de ellos gestionado por su servidor correspondiente. El cliente debe comunicarse con el servidor proxy para realizar cualquier petición, el cual ejerce de fachada aislando al resto de servidores. La organización de estos seria la siguiente:
	
	Para el objetivo de este proyecto, todos los servidores pueden ser tratados como una caja negra que se dedica al almacenamiento y distribución de datos, excepto el servidor proxy del cual estudiaremos en mayor profundidad su API.
	
5.1 Swift proxy server.
	La comunicación con este componente, se basa en el envio de peticiones HTTP. Dichas peticiones se gestionan mediante el uso de la operación GET, y deben contener unas cabeceras específicas para reconocer los distintos tipos de peticiones. Dichas peticiones disponen de una serie de restricciones detalladas a continuación:

El numero máximo de cabeceras por petición es 90.
La máxima longitud de las cabeceras es 4096 bytes.
Cada linea de petición no debe tener más de 8192 bytes.
Una petición no debe exceder el tamaño de 5 gigabytes.
El nombre de un contenedor no puede ser mayor de 256 bytes.
El nombre de un objeto no puede ser mayor que 1024 bytes.

	La operaciones posibles sobre el almacenamiento se dividen entre los tres servidores de gestión de este. Detallamos a continuación las operaciones relevantes:

Servidor de cuentas.
Listado de contenedores.
Servidor de contenedores.
Listado de objetos en el contenedor.
Creación de contenedores.
Eliminación de contenedores.
Servidor de objetos.
Recuperación de objetos.
Creación/actualización de objetos.
Copia de objetos.
Eliminación de objetos.

5.2 Operaciones en la API.
	Como ya adelantábamos, para interactuar con el servidor proxy se utiliza una interfaz REST, es decir, el servidor espera peticiones HTTP para servir cualquier tipo de fichero disponible, y realizar el control de acceso al almacenamiento.
5.2.1. Identificación.
	Esta operación es, por razones de seguridad, la primera a realizar de todas las posibles. Para ello hace falta proveer al servidor de un usuario válido y su contraseña a través de la operación GET de la versión 1.1 de HTTP. El servidor nos devolverá un identificador único necesario para realizar cualquier posterior operación. La solicitud es la siguiente:

	GET /<api version> HTTP/1.1
	Host: <Servidor proxy>
	X-Auth-User: <usuario>
	X-Auth-Key: <contraseña>
	La respuesta tiene la siguiente forma:
	HTTP/1.1 204 No Content
	Date: <fecha>
	Server: <servidor web>
	X-Storage-Url: <dirección para posteriores accesos>
	X-Auth-Token: <identificador para posteriores accesos>
	Content-Length: 0
	Content-Type: text/plain; charset=UTF-8

5.2.2. Listado de contenedores.
	La operación de listado de contenedores no requiere ningún parametro en especial, más que el indentificador de sesión obtenido durante la identificación, sin embargo, existen un par de variables configurables de interés general para dicha operación. Las variables se concatenan a la dirección solicitada en la operación GET, separando el listado de variables de la dirección por un interrogante('?'). Dichas variables sirven para describir el formato en que se recibirá el listado,siendo válidos los valores format=json y format=xml, el límite de contenedores que se quieren recibir, configurando la variable limit=N, y la coincidencia desde la cual quiere comenzarse el listado con la variable marker.
	
	GET /<api version>/<account>?var1=val1& … & varN=valN HTTP/1.1
	Host: <dirección del servidor>
 	X-Auth-Token: <identificador de sesión>
	Un ejemplo de respuesta en formato JSON es el siguiente:
	HTTP/1.1 200 OK
	Date: Tue, 25 Nov 2008 19:39:13 GMT
	Server: Apache
	Content-Type: application/json; charset=utf-8
	[
	  {"name":"test_container_1", "count":2, "bytes":78},
	  {"name":"test_container_2", "count":1, "bytes":17}
	] 
	En formato XML la respuesta sería de la siguiente forma:
	  HTTP/1.1 200 OK
	Date: Tue, 25 Nov 2008 19:42:35 GMT
	Server: Apache
	Content-Type: application/xml; charset=utf-8
	<?xml version="1.0" encoding="UTF-8"?>

	<account name="MichaelBarton">
	  <container>
	    <name>test_container_1</name>
	    <count>2</count>
	    <bytes>78</bytes>
	  </container>
	  <container>
	    <name>test_container_2</name>
	    <count>1</count>
	    <bytes>17</bytes>
	  </container>
	</account>
5.2.3. Listado de objetos.
	En el caso de los listados de objetos, se dispone de algunas variables de configuración adicionales, además de las ya descritas para los listados de contenedores. Estas nuevas variables son  end_marker, para devolver un listado terminado en el valor especificado, prefix, para ofrecer un prefijo que preceda al nombre del objeto y delimiter, para mostrar las ocurrencias listadas hasta el delimitador, desechando el resto de la dirección. Un ejemplo de uso de delimitador es aquel en el que solo queremos listar los directorios existentes en la raíz del almacenamiento, para lo cual solo requerimos utilizar el delimitador '/' para obtener el resultado deseado.
	El formato de la petición es el siguiente:

	GET /<api version>/<account>/<container>[?parm=value] HTTP/1.1
	Host: <servidor>
	X-Auth-Token: <identificador de sesion>
	Un ejemplo de respuesta en formato JSON:

	HTTP/1.1 200 OK
	Date: Tue, 25 Nov 2008 19:39:13 GMT
	Server: Apache
	Content-Length: 387
	Content-Type: application/json; charset=utf-8
	[
	  {"name":"test_obj_1",
	   "hash":"4281c348eaf83e70ddce0e07221c3d28",
	   "bytes":14,
	   "content_type":"application\/octet-stream",
	    "last_modified":"2009-02-03T05:26:32.612278"},
	  {"name":"test_obj_2",
	   "hash":"b039efe731ad111bc1b0ef221c3849d0",
	   "bytes":64,
	   "content_type":"application\/octet-stream",
	   "last_modified":"2009-02-03T05:26:32.612278"},
	]
	En formato XML el servidor respondería lo siguiente:
	HTTP/1.1 200 OK
	Date: Tue, 25 Nov 2008 19:42:35 GMT
	Server: Apache
	Content-Length: 643
	Content-Type: application/xml; charset=utf-8
	<?xml version="1.0" encoding="UTF-8"?>

	<container name="test_container_1">
	  <object>
	    <name>test_object_1</name>
	    <hash>4281c348eaf83e70ddce0e07221c3d28</hash>
	    <bytes>14</bytes>
	    <content_type>application/octet-stream</content_type>
	    <last_modified>2009-02-03T05:26:32.612278</last_modified>
	  </object>
	  <object>
	    <name>test_object_2</name>
	    <hash>b039efe731ad111bc1b0ef221c3849d0</hash>
	    <bytes>64</bytes>
	    <content_type>application/octet-stream</content_type>
	    <last_modified>2009-02-03T05:26:32.612278</last_modified>
	  </object>
	</container>
5.2.4. Creación de contenedores.
	Los contenedores, no son más que compartimentos donde almacenar los objetos, sin embargo, sus nombres deben cumplir las restricciones de no tener una longitud mayor de 256 caracteres, y no contener el carácter '/'. Un ejemplo de petición de creación de un contenedor es el siguiente:

	PUT /<api version>/<account>/<container> HTTP/1.1
	Host: <servidor>
	X-Auth-Token: <identificador de sesión>
	La respuesta en este caso:
	HTTP/1.1 201 Created
	Date: Thu, 07 Jun 2010 18:50:19 GMT
	Server: Apache
	Content-Type: text/plain; charset=UTF-8

5.2.5. Eliminación de contenedores.
	Para la eliminación de un contenedor, es suficiente con la siguiente petición:

	DELETE /<api version>/<account>/<container> HTTP/1.1
	Host: <server>
	X-Auth-Token: <identificador de sesión>
	Sin embargo deben suceder que el contenedor esté vacio para que este pueda ser eliminado. Su borrado es permanente. Su respuesta pudiera ser la siguiente:
	 HTTP/1.1 204 No Content
	 Date: Thu, 07 Jun 2010 18:57:07 GMT
	 Server: Apache
	 Content-Length: 0
	 Content-Type: text/plain; charset=UTF-8
5.2.6. Recuperación de objetos.
	Aunque habitualmente se utiliza la operación GET para recuperar objetos del almacenamiento masivo, es posible utilizar las directivas If-Match, If-None-Match, If-Modified-Since y If-Unmodified-Since definidas en el protocolo RFC2616. Como sus nombres indican, su funcionalidad es hacer condicional la recuperación del objeto en cuestión, definiendo si se encuentra un objeto que coincida en nombre, que no, o si ha sido modificado desde una fecha determinada.
	Existe también un soporte básico para recuperar rangos de memoria dentro del objeto, permitiendo recuperar solo partes de este si no se requiere el objeto completo.
	Su petición es la siguiente:
	
	GET /<api version>/<account>/<container>/<object> HTTP/1.1
	Host: <servidor>
	X-Auth-Token: <identificador de sesión>
	Su posible respuesta correspondiente sería:

	HTTP/1.1 200 Ok
	Date: Wed, 11 Jul 2010 19:37:41 GMT
	Server: Apache
	Last-Modified: Fri, 12 Jun 2010 13:40:18 GMT
	ETag: b0dffe8254d152d8fd28f3c5e0404a10
	Content-type: text/html
	Content-Length: 512000
	[ ... ]
5.2.7. Creación, o actualización de objetos.
	Es posible, y recomendable, que para la creación o actualización de objetos se utilizen sumas MD5 que ayuden a verificar la integridad de este. Está soportada dicha funcionalidad a través  del envío de una cabecera especial llamada Etag. Independientemente de si esta cabecera es incluida en la creación del objeto, al recuperarlo, siempre se devolverá el valor de la suma, para que el usuario pueda comprobar su integridad. Si se desea que el objeto expire en un tiempo determinado, o en una fecha, es posible añadir las cabeceras X-Delete-At y X-Delete-After. Su petición tiene la siguiente forma:

	PUT /<api version>/<account>/<container>/<object> HTTP/1.1
	Host: <servidor>
	X-Auth-Token: <identificador de sesión>
	ETag: <suma MD5>
	Content-Length: <longitud en bytes>
	X-Object-Meta-PIN: 1234
	[ ... ]

	Su posible respuesta sería:
	
	HTTP/1.1 201 Created
	Date: Thu, 07 Jun 2010 18:57:07 GMT
	Server: Apache
	ETag: d9f5eb4bba4e2f2f046e54611bc8196b
	Content-Length: 0
	Content-Type: text/plain; charset=UTF-8
5.2.8. Copia de objetos.
	De cometerse un error con el nombre del objeto, al subirlo, o sencillamente querer cambiar su nombre, debería eliminarse el objeto y volverlo a subir, produciendo sobrecarga en las comunicaciones. Esto se evita mediante la copia en servidor de objetos. Esta copia puede realizarse de dos formas. A través de la cabecera X-Copy-From, donde se especificará el contenedor y el objeto que se desea copiar, o mediante la operación COPY. Detallamos sus formas a continuación:

	PUT /<api version>/<account>/<container>/<destobject> HTTP/1.1
	Host: <storage URL>
	X-Auth-Token: <some-auth-token>
	X-Copy-From: /<container>/<sourceobject>
	Content-Length: 0 
	COPY /<api version>/<account>/<container>/<sourceobject> HTTP/1.1
	Host: <storage URL>
	X-Auth-Token: <some-auth-token>
	Destination: /<container>/<destobject>
5.2.9. Eliminación de objetos.
	La eliminación de objetos es inmediata y permanente, toda operación sobre el objeto despues de la operación DELETE, devolverá el conocido error 404 de HTTP, objeto no encontrado. Es posible programar la eliminación mediante el uso de las cabeceras X-Delete-At y X-Delete-After. La petición debe tener la siguiente forma:

	DELETE /<api version>/<account>/<container>/<object> HTTP/1.1
	Host: <servidor>
	X-Auth-Token: <identificador de sesión>
	Su respuesta se asemajará a la detallada a continuación:
	HTTP/1.1 204 No Content
	Date: Thu, 07 Jun 2010 20:59:39 GMT
	Server: Apache
	Content-Type: text/plain; charset=UTF-8
5.3 Entorno de operación.
	Para poder comprobar que nuestros drivers funcionan correctamente, es necesario preparar un servidor Openstack Swift donde poder realizar las conexiones pertinentes. Para este proyecto, hemos elegido utilizar una máquina virtual utilizando la tecnología de virtualización KVM, propia del núcleo de linux. La máquina ha sido dotada de un sistema operativo Debian GNU/Linux en su versión de pruebas(actualmente Debian GNU/Linux Wheezy), y dispone de dirección IP propia dentro de la red gracias a la compatibilidad de KVM con los puentes virtuales que ofrece Linux. Además del software básico que se instala automáticamente con la distribución, se le ha añadido un servidor OpenSSH para realizar conexiones seguras a la máquina y los paquetes, incluidos en los repositorios de la distribución, de Openstack Swift. De esta forma puede ejecutarse la máquina virtual como si de un servidor del sistema real se tratara, y podemos realizar cualquier prueba pertinente de nuestros drivers. Para la configuración del servidor Swift, hemos seguido las instrucciones detalladas en la documentación de Openstack SAIO(Swift All In One).

6. Linux.

	En el conocido sistema operativo libre, creado por Linus Torvalds y publicado en el año 1991, los programas en espacio de usuario deben comunicarse con el núcleo a través de módulos que permitan la gestión de dispositivos en el sistema. Dichos módulos representan tres tipos de dispositivos: de caracteres, de bloques, y de red. 
	Los dispositivos de caracteres son aquellos que se comunican mediante flujos de caracteres secuenciales, un ejemplo sencillo es el teclado, que puede ser accedido, y la información transmitida es un flujo de caracteres secuencial, si tecleas una palabra, esta se tratará en su estricto orden, y no en otro distinto. Los dispositivos de bloque suelen ser dispositivos de almacenamiento masivo y se estructuran en bloques o porciones que accedidas en secuencias de operaciones, y sus bloques no necesariamente se acceden de manera secuencial. Por último, los dispositivos de red, se encargan de cualquier tipo de comunicación con el exterior del sistema a través de sus interfaces de red, como las tarjetas Ethernet, modems, etc.
	Los dispositivos representados no tienen la obligación de ser dispositivos reales, pudiendo permitirse la existencia de dispositivos virtuales que resuelvan un tipo de tarea determinada. Un ejemplo claro de dispositivo virtual es el dispositivo de generación de números aleatorios(accedido desde el fichero /dev/random), el cual no requiere de ningún hardware especifico para desempeñar su función.
	La intención principal de Swiftness es proveer de un dispositivo de bloques que sea capaz de acceder al servidor Swift de Openstack, como si de un dispositivo local se tratara. Esto conlleva la clara diferencia de que, si detrás de un driver de dispositivo de bloques solemos encontrar dispositivos de almacenamiento físicos(como por ejemplo, un disco duro, una cinta magnética o un CD-ROM), en este caso nos encontraremos con una interfaz de red que debe saber comunicarse con el servidor.


	Cabría esperar que la labor de Swiftness fuera la de la traducción de las peticiones de bloques a directivas TCP, sin embargo dicha labor ya se encuentra cubierta por el módulo nbd(Network Block Device) que estudiaremos mas adelante. Con esto, la labor de Swiftness se reduce a implementar paquetes con las cabeceras adecuadas para que el servidor swift pueda ser gobernado por el módulo nbd.

6.1 Dispositivos de bloques.
	Los drivers dispositivos de bloques, dentro del código fuente de Linux, se alojan en el directorio drivers/block, y las interfaces que deben cumplir en el directorio include/linux. Como antes adelantábamos, dichos dispositivos disponen de grandes cantidades de información, dividida en bloques que no serán accedidos secuencialmente y es este hecho el que los diferencia de los dispositivos de caracteres. Debido a que tendremos que navegar a lo largo del dispositivo para acceder a la información, estos dispositivos ganan complejidad. La naturaleza de dichos dispositivos hacen al sistema altamente sensible a su disponibilidad.
	El tamaño de un sector depende del dispositivo en cuestión, y es la unidad minima fundamental de un dispositivo de bloques. Aunque muchos dispositivos de bloques dispongan de un tamaño de sector de 512 bytes, esto no quiere decir que sea un tamaño estándar. El sistema operativo debe cumplir con ciertas restricciones a la ora de acceder a un dispositivo de bloque. Debe acceder a la información en bloques de tamaño múltiplo del tamaño de sector, el tamaño de este bloque, deberá ser potencia de dos(restricción que suele aplicarse también al tamaño de sector),  y el tamaño de bloque no puede ser mayor que el tamaño de una página de información en memoria principal. Los tamaños más habituales son 512 bytes, 1 kilobytes, y 4 kilobytes.

6.1.1. Proceso de una operación.
	La jerarquía de subsistemas para acceder a un dispositivo de bloques, desde que una simple operación de lectura es enviada desde el espacio de usuario, hasta que llega al dispositivo es amplia. Esta contempla desde el sistema de ficheros virtual, hasta el driver del dispositivo de bloques, pasando por las caches de discos, los distintos sistemas de ficheros, conformando la capa de direccionamiento del sistema, la capa genérica del dispositivo de bloques, y el planificador de entrada salida. Debido a esto, resulta interesante estudiar que sucede, paso por paso, cuando una operación sobre el dispositivo está procesandose.
	Suponemos que se ha llamado a la rutina de servicio read() para hacer un acceso de lectura. Este acceso de lectura provoca que el sistema active la función más adecuada dentro del sistema de ficheros virtual para procesarla. Esta función recibirá el descriptor del fichero abierto a leer y un desplazamiento dentro del fichero, para describir el bloque que quiere leerse. El sistema de ficheros virtual, debe determinar si los datos están ya disponibles en memoria principal, y como se realizará el acceso. 
	De requerir acceder al dispositivo de bloques, el sistema de ficheros virtual acudirá a la capa de direccionamiento. En esta capa se ejecutarán dos tareas principales. La primera será determinar el tamaño de bloque, para calcular el desplazamiento dentro del fichero en función de este tamaño. Posteriormente se invoca una función del sistema de ficheros real que determine el nodo del fichero y su posición dentro del disco.
	Tras estas operaciones, el núcleo accederá a la capa genérica de dispositivos de bloque para comenzar la operación de entrada/salida. Aquí se crearán e inicializarán las estructuras necesarias que se ofrecerán al planificador de accesos.
	El planificador encola y ordena los accesos a los distintos bloques dentro del dispositivo con el fin de optimizar el acceso al dispositivo de bloques real, ya que, de una sola operación en el, accederemos a diversos bloques, y, debido a la lentitud del acceso a este tipo de dispositivos, es necesario asegurarse de que el número de accesos se reduzca todo lo posible.
	Finalmente, las peticiones se ofrecen al driver del dispositivo de bloques, que ejecutará la operación real. Todo este largo proceso puede verse resumido en la siguiente figura, donde se observa la jerarquía completa de subsistemas.


6.1.2. Estructura genérica de dispositivos de bloque.
	Cada vez que un bloque es alojado en memoria principal, este es asociado a un buffer. Un buffer es un objeto que representa un bloque en memoria, y, dado que el núcleo requiere más información que esta, cada buffer se asociará a un descriptor llamado buffer_head. Esta estructura almacenará datos como el estado del bloque, su página asociada, su tamaño, un puntero al comienzo de los datos, y el dispositivo al que pertenece, entre otros detalles. Puede consultarse la estructura en el fichero “include/linux/buffer_head.h”. Aunque mucha de esta información es necesaria para el núcleo, no deja de ser una sobrecarga y un consumo de espacio en memoria.
	Otra estructura de importancia, es aquella que almacena la información de una operación de entrada/salida. Esta es la estructura bio(block input/output, detallada en el fichero “include/linux/bio.h”). Las operaciones se descritas por esta estructura operan sobre segmentos, o porciones de un buffer, que no tienen la obligación de estar alojadas de forma contigua en memoria principal. De esta forma, el núcleo puede operar con un sector repartido a lo largo de toda la memoria. La información de esta estructura es mayoritariamente informativa, y sus campos mas relevantes con bi_vcnt, bi_idx y bi_io_vec. Estas estructuras se organizan en un vector de estructuras llamadas bio_vec. El campo bi_io_vec, almacena la dirección su vector asociado, bi_idx, su posición dentro del vector, y bi_vcnt, el número de estructuras bio_vec en el vector. Dicho esto solo queda detallar que las estructuras bio_vec se encargan de almacenar la información relativa a la página de memoria donde se aloja el segmento, su tamaño, y el desplazamiento dentro de la  página.
	Con esta organización, el subsistema de entrada/salida de dispositivos de bloques de Linux es capaz de describir operaciones, ordenarlas a conveniencia(normalmente por proximidad de páginas dentro del almacenamiento masivo), añadir y eliminar operaciones con relativa facilidad, y utilizar distintos esquemas de acceso al dispositivo de bloques. El panorama general se asemejara a la siguiente figura:


	La forma en la que el sistema de ficheros virtual maneja todas estas estructuras, es a través de una estructura request_queue, que, como cuyo nombre indica, es una cola de peticiones. Mientras la cola no este vacía, el driver del dispositivo de bloques ir retirando peticiones, o estructuras request, de la cabeza de la cola y enviándole dichas peticiones al dispositivo que soporta. La estructura request está compuesta de una o más estructuras bio que operan sobre bloques consecutivos en el dispositivo. Estas estructuras vienen definidas en el fichero “include/linux/blkdev.h”.
	Estas colas de peticiones serán manejadas por los distintos organizadores de las operaciones de entrada/salida. Entre los organizadores más importantes nos encontramos, el ascensor de Linus(Linus Elevator Scheduler), el organizador por plazos(Deadline Scheduler), el organizador anticipado(Anticipatory Scheduler), el organizador de cola completamente justo(Complete Fair Queue Scheduler), y el organizador de no operación(Noop Scheduler).

6.2 Dispositivos de red.
	Los dispositivos de red en Linux disponen de un comportamiento similar al de un dispositivo de bloques montado. Este debe registrarse en el núcleo del sistema operativo utilizando las estructuras habilitadas para tal efecto antes de que pueda transmitir o recibir ningún paquete. Aún así, parece razonable pensar que existen ciertas diferencias. Los dispositivos de red no disponen de un fichero en el directorio “/dev” que sirva de punto de acceso, puesto que sus operaciones principales ya no van a ser las de leer o escribir, sino las de transmitir o recibir. Además, utiliza un espacio de nombres distintos y provee al usuario de operaciones distintas, que operan sobre objetos diferentes en el núcleo. 
	Otra diferencia a subrayar es que un dispositivo de bloques actúa en consecuencia de una petición del núcleo, mientras que el dispositivo de red debe pedir al núcleo que procese el paquete que ha recibido asíncronamente. Esto implica que aunque el subsistema de red de este sistema operativo sea independiente del protocolo utilizado, su API sea completamente distinta.
	La forma en que un dispositivo de red se registra en Linux es insertar una estructura net_device dentro de una lista global de dispositivos de red. Esta estructura se detalla en el fichero “include/linux/netdevice.h”, y engloba toda la información relativa a la comunicación del dispositivo de red, además de información relativa a los diferentes protocolos soportados por el sistema. Los campos de dicha estructura más importantes son el nombre, su estado, la estructura net_device del siguiente dispositivo en la lista de dispositivos de red, los campos relativos a su espacio de memoria relacionado, su dirección, puerto, irq y canal de DMA.
	Para alojar dinámicamente la estructura se dispone de la función alloc_netdev, registrando el dispositivo en el núcleo. Sin embargo, tambien es posible hacer este registro mediante las funciones alloc_etherdev(Ethernet devices, “include/linux/etherdevice.h”), alloc_fcdev(Fiber-channel, “include/linux/fcdevice.h”), alloc_fddidev(FDDI devices, “include/linux/fddidevice.h”) o alloc_trdev(Token ring devices, “include/linux/trdevice.h”). La inicialización de las estructuras es realizada por dichas funciones, colocando unos parámetros por defecto que pueden ser modificados posteriormente. Dicha inicializacin la realiza la función ether_setup en el caso de llamar a alloc_etherdev.
	En el caso de descargar el módulo de la memoria, debe utilizarse las funciones unregister_netdev y free_netdev para liberar todos los recursos registrados durante la inicialización del modulo.
	Durante la apertura y cierre del dispositivo cabe destacar la necesidad de activar y desactivar el bit IFF_UP, del campo flag de la estructura netdevice, para cada correspondiente operación., la necesidad de copiar la dirección MAC del dispositivo a la estructura, y el uso de las funciones net_if_start_queue y net_if_stop_queue, para iniciar o detener las transferencias.
	En las transmisiones de paquetes se dispone de la función hard_start_xmit, que encola el envio de un paquete de datos almacenado en la estructura socket buffer(struct sk_buff, detallada en el fichero ”include/linux/sk_buff.h”). Cualquier paquete que se vaya a transmitir por la interfaz de red debe pertenecer a un socket, puesto que el almacenamiento de entrada/salida  de esta estructura son listas de estructuras sk_buff. El paquete debe de estar almacenado en la estructura tal y como deba aparecer en la interfaz física de red desde la que se vaya a enviar.
	Es necesario que durante las transmisiones de controle el acceso concurrente a la función hard_start_xmit. Para ello se utiliza el mecanismo de spinlock. Si el dispositivo dispone de memoria limitada, debe controlarse además la detención y reanudación de la cola de transmisión. Esto se realiza mediante las funciones netif_stop_queue y netif_wake_queue. 
	Dichos dispositivos deben contemplar la posibilidad de fallo en las transmisiones a través de temporizadores, sin embargo, no es necesario que sea el driver quien se encargue de preparar este mecanismo. El driver solo requiere indicar el periodo de espera del temporizador anotándolo en el campo watchdog_timeo de la estructura net_device. Esta temporización se mide en Jiffies. En caso de expirar este tiempo, se llama al método tx_timeout.
	La recepción de paquetes se controla de dos formas, mediante entrada/salida por interrupciones, o, en caso de redes de gran ancho de banda, por entrada/salida programada. Esto último se debe a que en este tipo de redes, la sobrecarga de pasar de contextos de ejecución, a un contexto de interrupción impide que se aproveche adecuadamente el ancho de banda disponible. 
	Para la recepción controlada por interrupciones, se define una función de recepción que utilize la función dev_alloc_skb para reservar espacio para un paquete en memoria. Esta función será llamada por el manejador de interrupción del dispositivo, contexto de interrupción, para almacenar el paquete en memoria principal, donde será procesado posteriormente.
	Es posible liberar el espacio de almacenamiento de un socket buffer mediante las funciones dev_kfree_skb(para contextos de ejecución), dev_kfree_skb_irq(para contextos de nterrupción) y dev_kfree_skb_any(para cualquier contexto).
	Para la entrada/salida programada, se utiliza una API diferente llamada NAPI(New API) que modela el comportamiento programado del dispositivo. Un driver que implemente esta API debe ser capaz de deshabilitar la interrupción de recepción de paquetes, manteniendo las interrupciones para las transmisiones válidas y otros eventos. Además, deberá implementar una función poll que se dedique a preguntar a la interfaz de red si ha recibido algo. En caso de no recibir nada, se reactivará la interrupción de recepción y se mantendrá al dispositivo a la espera.

6.3 Network Block Device.
	Como reza la documentación incluida en el núcleo de Linux, este módulo se encarga de conectar a un servidor remoto que exporte un dispositivo de bloques, con el fin de que la máquina cliente pueda acceder a este como si de un dispositivo local se tratara. Si establecemos una simple comparación con el objetivo de este proyecto, podemos constatar que la única diferencia con este, es que el servidor, no es uno propio, como el nbd-server, sino el servidor Swift de Openstack, luego es razonable pensar que podemos partir de este módulo para alcanzar nuestro objetivo.
	Este módulo envía las peticiones de un dispositivo de bloques genérico a través de la red, utilizando el protocolo TCP, para que el servidor la procese y envíe la respuesta al cliente de vuelta de la misma forma. Dichas peticiones, tienen la forma de estructuras request como las utilizadas por los dispositivos locales existentes en el sistema. Estas estructuras request se generan a raíz de las estructuras bio y bio_vec(comentadas en el apartado 5.1.2), tomando la información correspondiente al direccionamiento del bloque, y los bloques consecutivos que se quieren extraer.
	El servidor funciona en espacio de usuario completamente, siendo necesario dicho módulo solamente en el cliente.
	En el fichero <include/linux/nbd.h> podemos encontrar tres estructuras que definen el dispositivo de bloques, la petición y la respuesta. La estructura del dispositivo es la siguiente:

	struct nbd_device {
	int flags;
	int harderror;		/* Code of hard error			*/
	struct socket * sock;
	struct file * file; 	/* If == NULL, device is not ready, yet	*/
	int magic;

	spinlock_t queue_lock;
	struct list_head queue_head;	/* Requests waiting result */
	struct request *active_req;
	wait_queue_head_t active_wq;
	struct list_head waiting_queue;	/* Requests to be sent */
	wait_queue_head_t waiting_wq;

	struct mutex tx_lock;
	struct gendisk *disk;
	int blksize;
	u64 bytesize;
	pid_t pid; /* pid of nbd-client, if attached */
	int xmit_timeout;
	};

	Sus campos más destacables son el entero flags para configurar sus parámetros, la estructura socket que realiza el envio y recepción de peticiones, la estructura file que define el fichero del dispositivo y las dos colas de peticiones active_wq y waiting_wq. El driver se apoya en dichas colas para ejercer sus funciones. Una petición nueva, es encolada en la cola waiting_wq. Cuando llega su turno en la cola, la petición se envía y pasa a la cola active_wq para esperar su respuesta. Estas operaciones sobre las listas suceden tras la protección de concurrencia del mecanismo de spinlock. Ademas, con el fin de evitar problemas de concurrencia, se protege la transferencia de estructuras mediante un mutex.
	La estructura request de nbd es la siguiente:

	struct nbd_request {
	__be32 magic;
	__be32 type;	/* == READ || == WRITE 	*/
	char handle[8];
	__be64 from;
	__be32 len;
	} __attribute__((packed));

	Esta estructura dispone de un número mágico(o número único) para asegurar la consistencia de la petición, un tipo(lectura o escritura), el campo handle, que se utiliza para almacenar la dirección del manejador de la petición en curso, from, para almacenar la posición dentro del dispositivo donde se encuentra el fichero a tratar, y len, el tamaño de la petición, en multiplos de bloques.
	Finalmente la estructura reply se compone de lo siguiente:
	
	struct nbd_reply {
	__be32 magic;
	__be32 error;		/* 0 = ok, else error	*/
	char handle[8];		/* handle you got from request	*/
	};
	
	Podemos observar que se dispone de campos similares al de la estructura request, exceptuando que, en vez de denotar un tipo de respuesta, se almacena un posible código de error. Tampoco se requiere la posición de los bloques solicitados.
	De entre todas las funciones que conforman dicho driver, alojado en <drivers/block/nbd.c> nos interesan mayoritariamente las funciones que se encargan de transmitir y recibir paquetes a través de la red, y las funciones que creen o procesen dichos paquetes pues son las funciones susceptibles de modificación. Entre ellas, encontramos la función sock_xmit, que se encarga de transmitir un buffer utilizando el socket propio del dispositivo. Gracias a que su objetivo es muy simple, esta función no requiere ningún cambio, suponiendo una base de la comunicación imprescindible. Sin embargo, podemos observar las funciones sock_send_bvec y nbd_send_req, que se encargan de utilizar sock_xmit para enviar la estructura request y las estructuras bio_vec requeridas para procesar la operación en curso, y que con toda probabilidad deberán ser modificadas para permitir la comunicación con el servidor Swift. Estas dos funciones componen la comunicación en el sentido cliente-servidor.
	Las funciones sock_recv_bvec y nbd_read_stat son el equivalente a las funciones  sock_send_bvec y nbd_send_req en el sentido opuesto de la comunicación, es decir, la comunicación servidor-cliente. La primera de ellas recibe las estructuras bio_vec requeridas, y la segunda se encarga de procesar la respuesta. Con el diseño adecuado, dichas funciones no deberían ser susceptibles de cambio.
	La función nbd_handle_req añade la petición activa a la cabeza de la cola de peticiones activas, donde esperará su respuesta correspondiente. La función do_nbd_request será la encargada de encolar una nueva petición en la cola waiting_queue antes de ser enviada.
	Se definen en este módulo una serie de operaciones ioctl para trabajar con la estructura del dispositivo nbd, las cuales no resultan de especial interés para el cometido de este proyecto.

7. Windows.

	Dentro de la arquitectura del conocido sistema operativo de Microsoft nos encontramos con diversos componentes organizados de la siguiente manera:
	
	En la figura podemos diferenciar los subsistemas que trabajan en espacio de usuario y en espacio del núcleo. Dentro del espacio de usuario tendremos:

Procesos de soporte del sistema: ejemplos de este subsistema son el proceso de autenticación, el organizador de sesiones y otros procesos que no son iniciados por el organizador de control de servicios
Servicios del sistema y aplicaciones de usuario: estos se apoyan en librerías dinámicas del sistema, y los subsistemas de entorno para realizar su cometido.
Las propias librerías dinámicas: Su cometido es traducir la llamada al sistema realizada por el servicio, o la aplicación de usuario, a la función adecuada dentro del sistema. Para ello, si lo requiere, la librería enviará un mensaje al subsistema de entorno.
	
	En el espacio del núcleo, los componentes más importantes son:

El ejecutor de Windows, el cual conforma la base principal del sistema, ofreciendo los servicios mínimos del sistema(uso de memoria, creación de procesos e hilos, seguridad, entrada/salida...)
El núcleo, que reune las funciones en bajo nivel requeridas para que el ejecutor ofrezca los servicios mínimos
Los drivers de dispositivos, que se comunicarán con el hardware
La capa de abstracción de hardware, que aísla el núcleo del ejecutor de Windows. Como se puede observar, el sistema gráfico es independiente del bloque de sistemas antes comentado.

	Analizando en mayor detalle los subsistemas que componen el núcleo, podemos encontrar los siguientes componentes:
El manejador de entrada salida: dicho subsistema se apoya en los drivers de dispositivos o en los sistemas de ficheros para realizar delegar la tarea encomendada al componente correspondiente.
El manejador de cache, que se encarga de organizar los datos requeridos recientemente por el espacio de usuario en memoria, manteniendolos en esta para agilizar posteriores accesos.
El manejador de procesos e hilos, que se encarga de iniciar y finalizar los distintos procesos e hilos y de ofrecerles el soporte requerido.
El manejador de objetos del núcleo, cuyo objetivo principal es mantener las estructuras necesarias para su correcto funcionamiento.
El manejador de dispositivos Plug and Play, subsistema que detecta la aparición de un nuevo dispositivo en el sistema y pone en funcionamiento las infraestructuras necesarias del núcleo que permitirán su correcto funcionamiento.
El monitor de seguridad, que activará las politicas de seguridad de acceso a cualquier medio.
El manejador de memoria, que se encarga de alojar información en memoria principal, o devolverla al dispositivo de bloques del que proviene.
El manejador de configuración del sistema, que gestionará el registro de Windows y la informacion contenida en el.
El manejador de entrada/salida, que implementa una interfaz general de entrada salida independiente del dispositivo.
Las rutinas  de instrumentación ofrecidas por el servicio WMI de Windows, que permiten a los drivers de los dispositivos publicar información de configuración y estadisticas sobre el dispositivo.
El invocador de procedimientos avanzado, que realiza las tareas necesarias para la ejecución de cualquier software.
La interfaz de dispositivos de gráficos, apoyada en sus correspondientes dispositivos, y que realizará la tarea de gestionar el entorno gráfico del sistema.

	Estos subsistemas  están gobernados por el ejecutor de Windows y quedan resumidos en la siguiente imagen:
	

	Analizaremos posteriormente en mayor detalle los subsistemas más relevantes.

7.1 Subsistema de entorno y librerías dinámicas.
	Este componente ofrece aislamiento entre el espacio de usuario y el espacio del núcleo, sin llegar a pertenecer a este último. Actúa como un filtro de primera instancia donde una función de dichas librerías puede actuar de las siguientes formas:
La función se implementa en la librería, y por tanto, el espacio del núcleo no requiere saber de la llamada a esta función, luego será resuelta por la librería y se continuará con la ejecución normal del sistema.
La función requiere llamar al ejecutor de Windows para realizar alguna de sus funciones, luego se realizarán algunas tareas puntuales en espacio del núcleo y se continuará el resto de ejecución en espacio de usuario. 
La función requiere el uso del subsistema de entorno, enviándole un mensaje para que resuelva una tarea concreta, y continúe su ejecución posteriormente.

	Estos comportamientos se resumen en la siguiente figura:

	


	Entre los subsistemas de entornos, nos encontramos con el subsistema propio de Windows, y por cuestiones de soporte, el subsistema POSIX(Portable Operating System Interface based on Unix). Puesto que nuestro objetivo se centra en el espacio del núcleo, no estudiaremos en mayor profundidad este componente.

7.2 Ejecutor de Windows.
	El ejecutor de Windows conforma la primera capa de abstracción con la que una llamada al sistema se encontrará cuando se alcance el espacio del núcleo, haciendo de fachada para el resto del sistema. Está formado por numerosas funciones que pueden ser recogidas en los siguientes tipos:

Funciones exportadas al espacio de usuario, o servicios del sistema, la librería Ntdll se encarga de ofrecerlos. Muchos de los servicios son accesibles desde la API de Windows, o a través del subsistema de entorno.
Drivers de dispositivos, llamados a través de la función DeviceIOControl y que ofrece una interfaz general al espacio de usuario para llamar a los servicios de un dispositivo.
Funciones que solo pueden ser llamadas desde el espacio del núcleo. 
Funciones definidas como símbolos globales, pero que no son exportados al espacio de usuario.

	Además, el ejecutor de Windows contiene cuatro grupos de funciones de soporte para los componentes organizados por este. Los grupos son los siguientes:

Las funciones del manejador de objetos del ejecutor, que crea y destruye los objetos que sirven de infraestructura para el nucleo.
El ALPC(Advanced Local Procedure Call, lanzador de procedimientos locales avanzados) que se encarga del paso de mensajes entre procesos.
La librería de funciones comunes del sistema.
Las rutinas de soporte del ejecutor.

7.3 El núcleo.
	El núcleo consiste en una serie de funciones contenidas en Ntoskrnl.exe que ofrece los mecanismos principales para el funcionamiento del sistema. Es utilizado por el ejecutor como una arquitectura de hardware a bajo nivel y es completamente dependiente, por tanto, de la arquitectura donde se ejecute.
	Dentro de el se utilizan objetos de control y primitivas que serán controladas por el ejecutor, el cual expondrá los recursos compartidos utilizando las distintas políticas de seguridad. Además dispone de el control de region del procesador y el bloque de control para almacenar información especifica del procesador y su ejecución en curso.

7.4 Drivers de dispositivos.
	Los drivers en Windows conforman módulos que pueden cargarse en memoria principal bajo demanda. Pueden ser utilizados en contextos donde un hilo en espacio de usuario utiliza una de sus funciones, en contextos en los que un hilo en espacio del núcleo lo utiliza, o como resultado de una interrupción. Los drivers no utilizan los dispositivos directamente, sino que utilizan las funciones de la capa de abstracción de hardware(HAL, Hardware Abstraction Layer) para cumplir su cometido. Los tipos de drivers existentes son los siguientes:

Drivers de dispositivos, que utilizan la capa HAL para comunicarse con estos.
Drivers de sistemas de ficheros, que aceptan peticiones genéricas de entrada salida y las traducen a peticiones de un dispositivo concreto.
Drivers de filtro de sistemas de ficheros, los cuales son drivers que colaboran con los de sistemas de ficheros para ofrecerle una nueva funcionalidad.
Redirectores de red y servidores, que, siendo drivers de sistemas de ficheros, transmiten las peticiones de un sistema de ficheros a través de la red y reciben las peticiones.
Drivers de protocolos, los cuales implementan los protocolos de comunicaciones básicos y más extendidos como TCP/IP, NetBEUI e IPX/SPX.
Drivers de filtro para flujos de ejecución del núcleo, que implementan el procesos de señalización en grandes  flujos de datos.

	Con esto, el modelo de drivers puede ser resumido en tres tipos concretos:

Drivers de bus, que se encargan de utilizar los controladores de bus, adaptadores, puentes, y cualquier dispositivo capaz de comunicar diferentes dispositivos.
Drivers de función, que implementan la funcionalidad principal de un dispositivo en cuestión.
Drivers de filtro, que implementan una funcionalidad añadida a un driver de función.

	Para el desarrollo de estos drivers, la Windows Driver Foundation provee al usuario de las plataformas para drivers en espacio del núcleo(KMDF, Kernel-mode driver framework), y para drivers en espacio de usuario(UMDF, User-mode driver framework).

7.5 Componentes del sistema de entrada/salida.
	Dentro del sistema de entrada/salida existen una serie de componentes que colaboran habitualmente para ofrecer los servicios de los drivers al espacio de usuario. Estos componentes son el gestor de entrada/salida, el gestor del consumo energético, el gestor de dispositivos Plug and Play, y las rutinas de servicio para el proveedor de instrumentación de Windows.
	El gestor de entrada salida es el principal componente de este subsistema, cumpliendo el objetivo de conectar dispositivos(virtuales, lógicos y físicos) con el espacio de usuario. Conforma una infraestructura de soporte para los drivers.
	Los drivers de dispositivos ofrecen una interfaz de comunicación que es direccionada a través del gestor de entrada/salida. Cuando el usuario envía una petición, el gestor de entrada/salida la recoge y decide quien debe responder a esta, direccionando la petición al dispositivo adecuado. 
	El gestor de dispositivos Plug and Play permanece en contacto continuo con el gestor de entrada/salida y con los drivers de bus. Los drivers de bus le ofrecen alojamiento para los nuevos dispositivos, aparte de avisar de la detección de un nuevo dispositivo, o de su desaparición. Este subsistema cargará los módulos correspondientes y si no dispone de ellos, llamará a un gestor en el espacio de usuario.
	El gestor de consumo energético realizará las transiciones adecuadas de los dispositivos entre los distintos estados de consumo(encendido, apagado, modo de bajo consumo, etc).
	Las rutinas de servicio del proveedor de instrumentación de Windows hacen de interfaz de comunicación entre el proveedor de instrumentación y los drivers de los dispositivos.
	Además de estos componentes, es conveniente recordar que en este subsitema  utiliza el registro de Windows para mantener una base de datos de los dispositivos básicos, los ficheros INF, que sirven para almacenar la información de los drivers instalados en el sistema, y el nivel de abstracción de hardware.

7.5.1. Gestor de entrada/salida.
	Este subsistema sirve de infraestructura para recoger peticiones de entrada/salida, en forma de paquetes llamados IRP(I/O Request Packet). Su diseño permite que un hilo de procesamiento se encargue de multiples peticiones concurrentemente. El IRP contiene la información necesaria para describir la petición. El gestor de entrada/salida enviará un puntero a la dirección de memoria de la petición al driver correspondiente, donde será procesada. Este mismo puntero se utilizar para recuperar la información de la petición., cuando el driver notifique que la operación ha concluido.
	Por otro lado este subsistema ofrece un código común para todos los dispositivos, permitiendo que los drivers reduzcan su tamaño, retirándoles el código de las tareas comunes con otros dispositivos.
	Los drivers disponen de una interfaz modular y uniforme, de forma que el gestor de entrada salida puede tratarlos sin necesidad de tener ningún conocimiento acerca de como procesa las peticiones. De esta forma, el driver solo tiene que encargarse de traducir una petición genérica a una petición especifica del hardware que lo gestiona. Es posible que los drivers se llamen unos a otros para resolver la gestión de la petición en curso.
	Una operación típica de entrada/salida, es tratada por el gestor de entrada/salida, uno o dos dispositivos, y el nivel de abstracción de hardware. El sistema operativo, enmascara los dispositivos, de forma que el espacio de usuario dispondrá de las operaciones normales en un fichero cualquiera.

7.5.2. Drivers de dispositivos.
	Atendiendo a la clasificación de si se trata de un driver en espacio de usuario o en espacio de núcleo, podemos constatar que en el espacio de usuario se disponen de tres tipos: Drivers de Dispositivos Virtuales(VDDs), drivers de impresoras, y drivers de la infraestructura de drivers en espacio de usuario.
	Los drivers de dispositivos virtuales se mantienen por compatibilidad con MS-DOS, ya que emulan aplicaciones de 16 bits, capturando sus llamadas a dispositivos y traduciendolas a los dispositivos reales.
	Los drivers de impresoras traducen las peticiones de dispositivos de gráficos genéricos a comandos de impresión.
	Los drivers de la infraestructura de drivers en espacio de usuario son drivers de dispositivos que no requieren ejecución en espacio del núcleo, y que, en caso de necesitar algo, les es suficiente con comunicarse a través de las llamadas a procesos locales avanzados de Windows.
	En el espacio del nucleo, podemos se dispone de los drivers de sistemas de ficheros, que recogen las peticiones y las envian al dispositivo de bloques, o interfaz de red correspondiente, los drivers de dispositivos Plug and Play(PnP), entre los que se incluyen drivers de dispositivos de almacenamiento masivo, adaptadores de video, dispositivos de en entrada e interfaces de red, y drivers de dispositivos no Plug and Play, que conforman extensiones del núcleo, o nuevas funcionalidades para otros drivers.
	Por otro lado, se encuentran los drivers pertenecientes al modelo de drivers de Windows(Windows Driver Model). Entre estos encontramos los drivers de bus, los drivers de funcionalidad, y los drivers de filtro comentados anteriormente.
	Puesto que un driver puede ser dividido en distintos niveles, es posible clasificarlos en otros tres tipos distintos, de clase, de puertos o de minipuertos.
	Los dispositivos de clase implementan drivers para una clase en particular de driver(drivers de discos, de cintas, de CR-ROM...).
	Los dispositivos de puertos procesan peticiones para enviarlas por un tipo específico de puerto, como un puerto SCSI, COM, USB. Estos se implementan como librerías del núcleo, ya que son drivers escritos por Microsoft con el sistema operativo.
	Los drivers de minipuertos traducen una petición genérica a un petición de un puerto concreto, siendo drivers para un dispositivo que importan las funciones de un dispositivo de puerto.
	Si una petición debiera ser atendida por un driver con diferentes niveles, el gestor de entrada/salida recogería la petición del espacio de usuario, la traduciría a una petición para el driver que conforme el primer nivel, con su respuesta, generaría una nueva petición para el siguiente nivel, y seguiría generando posteriores peticiones sucesivamente hasta llegar a la última. Una vez respondida la última petición, se devolvería la respuesta al espacio de usuario, conformando una cola de peticiones por nivel.

7.5.3. Estructura de un driver.
	Dentro de un driver se encuentran una serie de funciones que resuelven las diversas tareas que requiere el sistema operativo para ofrecer el resultado de las operaciones al espacio de usuario. Pueden dividirse en dos subgrupos, las funciones principales, que se encuentran en todo driver, y las funciones auxiliares, que según las infraestructuras que requiera el driver, aparecerán, o no lo harán.
	Dentro de las rutinas principales, o imprescindibles tenemos las rutinas de inicialización del driver(DriverEntry) que se utilizan recien cargado el driver en memoria principal, e inicializa las estructuras principales del driver.
	Para el buen funcionamiento de la infraestructura PnP se incluye una rutina de añadido de dispositivo(add-device routine) que recibe la notificación del gestor PnP y aloja el objeto del dispositivo descubierto en el núcleo.
	Es posible encontrar una serie de rutinas dentro del driver que conforman las operaciones que puede realizar el usuario sobre el dispositivo. Las principales suelen ser las rutinas de apertura, cierre, lectura y escritura, aunque pueden existir otras en función de las operaciones que ofrezca el dispositivo.
	Si el driver se sirve de las colas de gestión proveidas por gestor de entrada/salida, definirá una operación start que iniciará la transferencia de peticiones. El gestor de entrada/salida se encargará entonces de serializar las peticiones y enviarlas una a una al dispositivo.
	Por último se definen dos funciones relacionadas con el uso de interrupciones, la rutina de servicio de interrupción(ISR), y la rutina de resolución para el servicio de interrupción.  La primera completará las labores mínimas de procesado, pues su ejecución debe ser lo más rápida posible. Habitualmente encola la petición, para que sea procesado posteriormente en la rutina de proceso del servicio de interrupción.
	El resto de rutinas que detallamos a continuación, pertenecen al segundo grupo antes comentado, es decir, estas rutinas aparecen en función de ciertas condiciones que debe cumplir el driver, y de no cumplirse no existe la necesidad de implementarla.
	Si el driver dispone de diferentes niveles encontraremos en este al menos una, aunque pueden ser varias, rutinas de finalización. Estas rutinas avisan a los niveles superiores de la resolución de la operación en niveles inferiores, y del estado de esta(correcta, incorrecta, cancelada).
	El driver puede ofrecer una o varias rutinas de cancelación. Si la petición puede ser cancelada, esta incluirá la rutina que debe ejecutarse en caso de recibir una petición de cancelación. Esta rutina realizará labores de limpieza para que el driver pase a un estado consistente para procesar nuevas peticiones.
	Si el driver está preparado para comunicarse con el gestor de cache, debe implementar una rutina de resolución rápida. Esta rutina pemitirá resolver la petición a través de la cache del sistema, evitando realizar nuevas operaciones de entrada salida.
	Es posible, pero no necesario, que el driver disponga de una rutina de descarga, para realizar las labores de liberación de memoria y recursos que este utilizara durante su ejecución. Además, es posible que definir una rutina, aparte, para la limpieza de las infraestructuras en el apagado del sistema.
	Por último, puede definirse una rutina de registro de errores.

7.5.4. Objetos de drivers y objetos de dispositivos.
	El gestor de entrada/salida se comunica con dos tipos de objetos que le ofrecen las operaciones disponibles y cualquier información que requiera saber para llevarla a cabo. Estos son los objetos de driver, y de dipositivo. Los objetos de drivers representan el driver que puede manejar los distintos dispositivos conectados al sistema, y los de dispositivo, los dispositivos, físicos o lógicos que pueden ser gobernados por dicho driver.
	Cuando el driver es cargado en memoria, el objeto de driver es creado. Inmediatamente después se llamará a la rutina de inicialización. A partir de entonces es posible crear objetos de dispositivo gobernados por dicho driver. Para ello se utilizan las operaciones IoCreateDevice e IoCreateDeviceSecure, aunque habitualmente los dispositivos utilizarán la rutina add-device y la infraestructura Plug and Play.
	Al crearse el dispositivo, es posible asignarle un nombre que será almacenado en el gestor del espacio de nombres de objetos, el cual es inaccesible directamente desde el espacio de usuario. Para que sea accesible, debe hacerse un enlace desde el directorio \Global hacia el directorio \Device, del espacio de nombres, donde el nombre del objeto realmente se encuentra. Para exportar las interfaces  se utiliza la función IoRegisterDeviceInterface, y para habilitarla, se utiliza IoSetDeviceInterfaceState. Desde ese momento, el usuario puede utilizar la infraestructura PnP para utilizar la interfaz.
	Con esta infraestructura, un objeto driver es asociado con los distintos objetos dispositivos. De esta forma, el gestor de entrada/salida no requiere conocer los detalles de un dispositivo concreto.

7.5.5. Estructuras de ficheros.
	Las estructuras de ficheros cumplen los requisitos de diseño de los objetos en Windows(son recursos compartidos, pueden tener nombre, deben estar protegidos y soportan sincronización). Su única diferencia, es que los objetos compartidos suelen estar alojados en memoria, y en los ficheros, se encuentran en un dispositivo físico. Su representación se compone de los siguientes atributos:

Nombre de fichero.
Desplazamiento del byte actual(respecto al comienzo del fichero).
Modo de compartición.
Banderas de modo de apertura.
Puntero al objeto de dispositivo.
Puntero a la partición.
Puntero a la sección de punteros de objetos.
Puntero al mapa privado de cache.
Lista de paquetes de petición de entrada/salida.
Contexto de finalización de entrada/salida: de existir, comunica el estado de finalización de la petición en curso sobre el puerto
Extensiones del objeto de fichero: almacena la prioridad, si requiere chequeos de seguridad, y si contiene extensiones que almacenan información de contexto.

	El nombre y el desplazamiento se utilizarán para posicionarse en la lectura secuencial del fichero. El modo de compartición y las banderas de modo de apertura controlan la seguridad de los accesos. El puntero al dispositivo, a la partición, y a la sección de punteros de objetos identifican el dispositivo físico que aloja el fichero, el mapa de cache y la lista de IRPs para procesar las peticiones. Las extensiones del objeto de fichero pueden ser las siguientes:

Parámetros de transacción.
Detalles del objeto de dispositivo: Identifica el driver de filtro adecuado.
Estatus de bloqueo de rango de entrada/salida: permite al espacio de usuario bloquear buffers en el espacio del núcleo optimizando las operaciones asíncronas.
Genérica: almacena información específica para el driver de filtro.
Entrada/salida programada del fichero: almacena la reserva de ancho de banda del dispositivo para garantizar el buen funcionamiento de aplicaciones multimedia.
Enlaces simbólicos: información para resolver enlaces simbólicos.

	Cuando el espacio de usuario pide al núcleo la apertura de un fichero, el gestor de entrada/salida se comunica con el subsistema adecuado(en este caso, el gestor de objetos), para que este, con ayuda del núcleo genere un manejador para el fichero. El gestor de entrada/salida, enviará posteriormente este manejador ala aplicación que la solicitó. Durante este proceso, cualquier chequeo de seguridad debe ser resuelto por el gestor de entrada/salida.
	El objeto de fichero es alojado en memoria y es una representación de un recurso compartido, no el recurso en sí. Esta representación almacena los datos necesarios para utilizar el fichero, mientras que el fichero almacena los datos a utilizar. Por otro lado, el manejador debe ser único para un proceso, mientras que el fichero no tiene por que serlo(p.e. Varios procesos utilizan el mismo fichero). En caso de escrituras, se evitan los problemas de concurrencia utilizando la función de Windows LockFile.
	Cuando el fichero se abre, se incluye en el espacio de nombres bajo el directorio “\Device\HarddiskVolumeN”, que representa el directorio de la partición abierta, de la que se recibe el fichero. Para que sea compartido directamente con el espacio de usuario, es necesario hacer un enlace simbólico en el directorio “\Global”.

7.5.6. Paquetes de peticiones de entrada/salida.
	Las IRPs son las estructuras que Windows utiliza para almacenar la información requerida para realizar una operación sobre un dispositivo. Si un hilo utiliza un servicio de entrada/salida, el gestor de entrada salida generará el IRP correspondiente a la operación solicitada. El sistema gestiona las peticiones utilizando dos colas y una pila, una cola corta para las operaciones que solo utilizen una posición en la pila, y una cola larga para las que utilizen múltiples posiciones en la pila. Para mejorar la gestión de operaciones entre varios procesadores, se añade una lista global además de las anteriores.
	Por defecto un IRP utiliza 10 posiciones en la pila de IRPs, y, por tanto, se aloja en la lista larga de IRPs. Una vez por minuto, el sistema ajusta cuantas posiciones de la pila requiere, llegando a permitir un máximo de 20 posiciones. Despues de su alojamiento e inicialización, el gestor de entrada/salida almacena un puntero al solicitante del servicio en el IRP.
	Dentro de un IRP se diferencian dos partes, una cabecera fija y una o varias posiciones en la pila. La información del tipo de operación se almacena en la cabecera, mientras que en las posiciones de la pila se almacena código de una función(llamada función mayor), sus parámetros, y el solicitante del objeto fichero. La función mayor indica que rutina de resolución del driver debe ser llamada por el gestor de entrada/salida cuando el paquete se envie al driver. Opcionalmente, puede almacenarse una función(llamada menor) para que modifique el comportamiento de la función mayor(añadiendo funcionalidades, comunicando con otros subsistemas, etc).
	Habitualmente se ofrecer rutinas de resolución para las operaciones más habituales(apertura, cierre, lectura, escritura, control de entrada/salida, Plug and Play y alguna rutina para WMI).
	Una vez almacenado el IRP en la lista correspondiente, el sistema es capaz de encontrarlo y cancelarlo en caso de descontrol.
	La creación de un IRP se realiza utilizando los servicios NtReadFile, NtWriteFile, o NtDeviceIoControlFile. El gestor de entrada/salida decide si debe participar en la gestión de los buffers de almacenamiento requeridos por la operación. Puede actuar de tres formas. La primera de ellas sería almacenar un buffer del mismo tamaño que el que ofrece la aplicación que solicita el servicio. En caso de escritura, copia el buffer a su almacenamiento privado, y en el caso de lectura copia la información de su buffer al espacio de usuario.
	Otra opción es realizar entrada/salida directa, bloqueando el buffer en espacio de usuario para que no pueda ser escrito, y desbloqueandolo cuando se finalice la operación.
	Por último el gestor puede sencillamente no ejercer ninguna gestión de almacenamiento. En este caso, debe ser el driver quien se encargue de estas labores.
	En cualquier caso, el gestor de entrada/salida inicializará las referencias adecuadas para localizar el almacenamiento. El tipo de gestión utilizado es seleccionado en función de lo que solicite el driver, el cual registra el tipo deseado para la lectura y escritura, mientras que para el control se almacena en un código de control específico.
	Usualmente, para transferencias menores que una página de memoria(4KB) se utiliza la entrada/salida a través de buffers, y para transferencias mayores la entrada/salida directa. Es poco común utilizar la gestión de almacenamiento desde el driver por la posibilidad de que se pierda la referencia al solicitante. Su uso de reduce entonces a drivers en espacio de usuario que garantizan que las referencias son válidas, y pertenecientes al espacio de usuario

7.5.7. Petición en drivers de una sola capa.
	Este proceso conlleva siete pasos:

Envío de la petición a través del subsistema de DLLs.
El subsistema de DLLs solicita el servicio NtWriteFile.
El gestor de entrada/salida crea el IRP correspondiente y lo envia al driver haciendo uso de IoCallDriver.
El driver transmite el IRP al dispositivo y comienza la operación de entrada/salida.
El dispositivo notifica la finalización del servicio mediante una interrupción.
El driver sirve la interrupción.
El driver llama al servicio IoCompleteRequest.

	Las interrupciones se sirven en dos partes diferenciadas. Una vez ocurrida, el gestor de interrupción reconoce el estado y  razón de la interrupción para encolarlo en una cola de resolución, donde se gestionará su finalización en un segundo plano, permitiendo que sucedan nuevas interrupciones rápidamente.
	La rutina de finalización se encarga de copiar la información pertinente hacia el espacio de usuario, y limpiar las estructuras requeridas para gestionar una nueva petición.
	Debe cuidarse la sincronización al acceder al driver, debido a que el sistema permite que un hilo de mayor prioridad expulse a uno de menor prioridad de su ejecución, que se atienda una interrupción durante esta, o que se ejecute el mismo código en otro procesador de la misma máquina. De esta forma, la información compartida por el driver debe ser consistente para cualquier ejecución en curso.

7.5.8. Petición en drivers de múltiples capas.
	En este ocasión, el gestor de entrada salida, al igual que en el caso anterior genera un IRP que se pasará al driver del sistema de ficheros. Dependiendo de la petición, el driver la reenvia al driver del dispositivo en cuestión, o genera IRPs adicionales para enviarlas por separado al driver. Solo se reutiliza un IRP en caso de que se traduzca en una sola petición al dispositivo.
	Como alternativa a reutilizar una petición, el sistema de ficheros puede generar un grupo de IRPs asociados que trabajen en paralelo para atender una sola petición. Este grupo es transmitido al gestor de volumenes, que lo enviará posteriormente al driver del dispositivo.

7.5.9. Infraestructura de drivers en espacio del núcleo(KMDF).
	El modelo de drivers en espacio de núcleo no es muy distinto del modelo de drivers WDM. Esto implica que en su interior disponen de las funciones de inicialización, la rutina de añadido de dispositivo al sistema, y una o más rutinas de eventos, que funcionan como las rutinas de resolución de lo drivers pertenecientes a WDM. Generalmente, estas últimas, crean y gestionan colas de peticiones para la entrada/salida. El driver no tiene la obligación de implementar estas rutinas, puesto que existen rutinas genéricas proveídas por el sistema operativo. En el caso de implementarlas, la rutina debe ser registrada por el sistema operativo para que pueda ser utilizada.
	El modelo de datos de KMDF, es orientado a objetos. Estos objetos se encapsulan internamente, permitiendo solo la interactuación con ellos a través de una interfaz de manejadores. La infraestructura ofrece rutinas para la creación de estos objetos, recuperación de valores, y actualización de estos(WdfDeviceCreate, Get/Set, Assign/Retrieve).
	Los objetos de drivers pertenecen a una jerarquía, donde el objeto raíz es el objeto WDFDRIVER, que describe el driver actual, y que es análoga al driver de objeto ofrecido por el gestor de entrada/salida. Todo objeto generado por KMDF para este driver será un hijo de este dentro de la jerarquía. Dentro de la jerarquía podemos destacar los siguientes objetos:

WDFDRIVER: Objeto raíz de la jerarquía.
WDFREQUEST: Objeto auxiliar de petición.
WDFIORESREQLIST: Lista de objetos de rangos de direcciones para la entrada/salida(Resource Requirements).
WDFIORESLIST: Identifica un rango de direcciones para el dispositivo.
WDFDEVICE: Objeto del dispositivo.
WDFCHILDLIST: Lista de objetos hijos del dispositivo asociado.
WDFFILEOBJECT: Objeto fichero.
WDFIOTARGET: Dispositivo  que debe resolver la petición de entrada/salida.
WDFKEY: Elemento del registro.
WDFINTERRUPT: Instancia de interrupción.

Dicha jerarquía se resume en la siguiente figura:

	Es importante resaltar que se contempla el contexto de un objeto, de forma de que, aunque los objetos sean opacos, estos permiten al driver almacenar su propia información en su objeto padre para mejorar la localización de la información. De hecho, es posible disponer de más de un área de contexto, permitiendo a múltiples niveles de código interactuar con el mismo driver. De esta forma, los niveles trabajan de forma independiente.

7.5.10. Modelo de entrada/salida de KMDF.
	El modelo de entrada/salida de KMDF utiliza los mismos mecanismos que WDM, las interfaces de WDM y la API del núcleo, de forma que el mismo puede ser observado como un driver de la WDM en sí mismo. Basándose en el modelo de WDM el driver de KMDF realizará alguna de las siguientes tres operaciones:

Enviar el paquete de petición al gestor de entrada/salida.
Enviar el paquete al sistema de Plug and Play y el gestor de consumo para procesarlo y modificar el estado de consumo si es requerido.
Enviar el paquete al gestor de servicios WMI.

	Estos componentes notificarán al driver los eventos registrados para direccionar la petición hacia el manejador adecuado. De haber terminado KMDF su proceso del IRP, sin haberse satisfecho la petición, KMDF optará entre completar la petición con estado STATUS_INVALID_DEVICE_REQUEST, en caso de tratarse de un driver de bus, o de función, o redireccionar la petición al driver de nivel inferior en caso de utilizar drivers de filtro multinivel.
	El proceso de paquetes de petición, una vez más, sigue basándose en colas(WDFQUEUE). Esto le ofrece la oportunidad de ordenarlas como requiera
	Un driver típico de KMDF crea una cola y le asocia uno o varios eventos, además de los callbacks que requiera, los estados de consumo, los métodos de resolución para la cola y si aceptará, o no, buffers vacíos.
	Las tareas gestionadas por el sistema de gestión de entrada/salida de KMDF(aparte de las habituales de creación, cierre, liberación de recursos, lectura, escritura y control) son:
La creación de peticiones, notificada a través del evento EvtDeviceFileCreate. Otra opción es crear una cola automática y registrar un callback para el evento EvtIoDefault. De no utilizarse alguno de estos métodos, KMDF devolverán un código de finalización correcta.
En caso de liberación de recursos y peticiones de cierre, se notificarán los eventos EvtFileCleanup y EvtFileClose respectivamente, y se llamarán a sus callbacks correspondientes.

7.5.11. Gestor de dispositivos Plug and Play.
	Este gestor ofrece la habilidad de reconocer y adaptar los cambios de configuraciones de hardware,  instalando y eliminando dispositivos. Para ello se sirve de la cooperación con el hardware, su driver, y los distintos niveles dentro del sistema operativo. Además, se apoya sobre un estándar de identificación de dispositivos alojados en los distintos buses. Este estándar ofrece las siguientes capacidades al sistema de PnP:

Reconocimiento automático de dispositivos inñstalados(enumerando los existentes en el proceso de inicio y detectando los añadidos y eliminados después de este).
Alojamiento de recursos requeridos por el hardware(reuniendo los requerimientos, y asignándolos(o reasignándolos si es necesario) a través del arbitrador de recursos.
Carga del driver requerido en memoria. Si este driver no estuvieera instalado, el subsistema de PnP en espacio del núcleo pedirá al subsistema de PnP en espacio de usuario que instale el dispositivo, con ayuda del usuario.
Mecanismos de detección y configuración de hardware para drivers y aplicaciones.
Almacenamiento para los estados del dispositivo.
Gestión de dispositivos conectados en la red.

	Es importante resaltar que tanto el driver, como el dispositivo deben soportar PnP, de no hacerlo alguno de los dos, el sistema puede verse comprometido. Es posible, aún así, realizar un soporte parcial del sistema PnP sobre un dispositivo que no lo soporte, si el driver si implementa los requerimientos de este sistema.
	Para ofrecer soporte al sistema, debe implementarse una rutina de resolución para este sistema, otra para el sistema de control de consumo, y una rutina de añadido del dispositivo. Es importante saber que el soporte de PnP en un driver de bus, será distinto que en un driver de función o de filtro. El driver de bus debe describir de forma única los dispositivos alojados en el bus que representa, cargar su driver de función y llamar a la rutina de añadido de dispositivo.
	Los drivers de función y de filtro, deben prepararse para utilizar el dispositivo en la rutina de añadido del dispositivo, sin necesidad de comunicarse con el. Para comunicarse espera a que el gestor de PnP le envíe un comando de comienzo del dispositivo para pasar el control a la rutina de resolución adecuada. Es entonces cuando se configura el dispositivo. Si una aplicación trata de abrir el dispositivo que no ha finalizado de iniciarse recibirá un código de error indicando que el dispositivo no existe.
	Una vez inicializado el dispositivo, el gestor de PnP puede enviar diversos comandos, como por ejemplo, el de eliminación del dispositivo, o reasignación de recursos. Estos comandos guiarán al driver a través de una tabla de transición de estados bien definida.

7.5.12. Carga, inicialización e instalación de drivers.
	Existen dos tipos de inicialización, la explícita, y la basada en enumeración(utilizando los servicios de Windows). Todo driver dispone de valores de registro en la rama de servicios del conjunto de control actual. Estos valores definen el tipo de imagen, la localización del driver, los valores de control, o el orden de inicialización del servicio. Las diferencias entre la carga explicita frente a la carga a través de servicios de Windows son que el driver puede especificar un valor de comienzo para el comando boot-start(0, se carga durante la inicialización del subsistema de ejecución) o system-start(1, se carga después de la inicialización del subsistema de ejecución), y que el driver puede usar los valores de control Group y Tag para ser ordenados en la fase de inicio, pero no pueden usar los valores DependOnGroup o DependOnServices. El valor de registro Group se compara con el valor de registro “HKLM\SYSTEM\CurrentControlSet\Control\ServiceGroupOrder\List” para determinar en que orden de inicializan los grupos durante el inicio. Para refinar el orden de carga dentro del grupo se utiliza la etiqueta Tag y el registro “HKML\SYSTEM\CurrentControlSet\Control\GroupOrderList” que define la preferencia dentro del grupo.
	Las guías utilizadas para otorgar un valor de comienzo son las siguiente:

Drivers no Plug and Play reciben el valor de carga en inicio que requieran.
Cualquier driver(no-PnP o PnP) que requiera inicializarse durante la carga del sistema operativo recibe el valor boot-start(0).
Si no es requerido para ejecutar el sistema operativo y el driver detecta un dispositivo que los drivers de bus no pueden enumerar, recibe el valor de system-start(1).
Drivers no-PnP y de sistemas de ficheros que no requieren estar presentes en la ejecución del sistema operativo reciben un valor de auto-start(2).
Los drivers PnP que o se requieren en el inicio del sistema operativo reciben un valor de demand-start(3).

	Estos valores solo son útiles durante la carga del sistema operativo.
	Para la enumeración de dispositivos, se hace uso de un driver virtual no-PnP de bus que ejerce de raíz, para representar el sistema, y la capa HAL, comentada anteriormente, y que se nutre de las descripciones ofrecidas por el registro para detectar el bus primario. Dentro de este bus pueden encontrarse dispositivos y otros buses(como el bus USB, PCI, PCI-e...), en los que pueden encontrarse otros dispositivos, de forma que la enumeración pueda realizarse de forma recursiva, formando un árbol de dispositivos. Los nodos en el árbol se llamaran devnodes, y contienen objetos de dispositivos. Los buses alojados en el sistema deben cumplir la interfaz ACPI para el control de consumo. La herramienta para poder ver el árbol de dispositivos es el gestor de dispositivos(Device Manager).
	La carga e inicialización de dispositivos sigue los siguientes pasos:

El gestor de entrada/salida invoca la rutina de inicialización del driver, para todo driver que deba cargarse en el inicio del sistema. Si dicho driver tiene dispositivos que atender, el gestor de entrada/salida los enumera y notifica su presencia al gestor de PnP. Si se encuentra un dispositivo que no dispone de un driver de carga en el inicìo, se crea la estructura devnode, pero ni se carga su driver, ni se inicializa la estructura.
Después de la carga de drivers en el inicio, el gestor de PnP recorre el árbol de dispositivos, cargando los drivers para las estructuras devnodes que no se cargaron en el paso anterior, y enumera todo dispositivo que sea atendido por el driver. En este paso, se obvia cualquier valor de precedencia para los dispositivos. Al finalizar este paso, todo dispositivo PnP esta listo para usarse.
Se comienza la carga de los drivers de carga durante el inicio del sistema, que no hayan sido cargados ya. Los dispositivos detectados por estos drivers son no numerables, y el gestor de PnP no iniciará los dispositivos hasta que los drivers enumerados sean iniciados y configurados.
El gestor de servicios de control carga los drivers con valor auto-start.

	El árbol de dispositivos servirá de guía para el gestor de Plug and Play y el gestor de consumo. Los dispositivos quedarán registrados en le entrada “HKLM\SYSTEM\CurrentControlSet\Services”. La forma en que se almacenan los valores de los dispositivos es “Enumerado\Id. de dispositivo\Id. de instancia”.
	Los devnodes se componen de al menos dos objetos de dispositivos. Uno de ellos es un objeto de dispositivo físico(Physical device object, o PDO) que el gestor de PnP pedirá crear al driver de bus cuando encuentre un dispositivo que esté presente en este. Otros de los objetos son los objetos de filtro de dispositivos(filter device objects, o FiDOs) que ofrecen una interfaz entre el dispositivo físico y los objetos de dispositivos funcionales o FDO, que se crean en el driver y será utilizado por el gestor de PnP para gestionar el dispositivo. Además, los objetos de función de driver, pueden crear algunos objetos de filtro que cumplan una función a nivel superior a este.
	

	Aunque exista una pila de objetos para controlas el dispositivo, cualquier petición puede ir dirigida a cualquier nivel, y no es necesario que pase ni por los niveles superiores, ni por los	
 inferiores.
	Para la carga de dispositivos, el gestor de PnP de apoya en el registro. Al enumerarse, el identificador de dispositivo es enviado al gestor junto con un identificador de instancia. Con esta información, el gesto genera una instancia de dispositivo para almacenar los valores dentro de la rama de enumeración en el registro. El valor de clase(ClasssUID) se almacena en “HKLM\SYSTEM\CurrentControlSet\Control\Class”.
	El orden de carga de dispositivos se apoya en las siguientes reglas:

Primero se cargan los drivers de filtro de bajo nivel especificados en la enumeración de dispositivo.
Después se cargan los drivers de filtro de bajo nivel especificados en la clase de dispositivo.
A continuación se cargan los drivers de función especificados en la enumeración de dispositivos.
Posteriormente se cargan los drivers de filtro de alto nivel especificados en la enumeración de dispositivos.
Por último se cargan los drivers de filtro de alto nivel especificados por la clase de dispositivo.

	Todo drivers quedará referenciado en la clave de registro “HKML\SYSTEM\CurrentControlSet\Services”.
	En cuanto a la instalación de drivers, de no disponerse del driver adecuado para utilizar un dispositivo, deberá solicitarse al gestor de PnP en espacio de usuario. Si se ha detectado en el inicio del sistema, se definirá su estructura devnode, pero su carga será postpuesta hasta que el gestor en espacio de usuario resuelva la problemática. Varios componentes entran en juego en este proceso, algunos proveidos por el sistema, y otros por el instalador del driver.
	El proceso de instalación comienza en la notificación del driver de bus al gestor de PnP. Este busca en el registro de Windows el driver de función correspondiente, y al no encontrarlo, informa al gestor de PnP en espacio de usuario, enviándole el identificador de dispositivo. Este gestor, intentará una instalación sin la intervención del usuario, que de no tener éxito llamará al ejecutale Rundll32.exe para que se inicie la interacción con el usuario, con la correspondiente comprobación de privilegios. El instalador utilizará el sistema Setup y CfgMgr para detectar el fichero INF que guarda la información de instalación del driver. Inicialmente lo buscará en el almacén de drivers(%SYSTEMROOT%\System32\DriverStore). Si no dispone de él, se le pregunta al usuario para que lo provea, instalándolo en el almacén de drivers y ejecutando posteriormente Drvinst.exe para que se guarde constancia del nuevo driver en el registro.
	Desde entonces, localizar el driver de un dispositivo, es un proceso en el cual se pide a los dispositivos de bus una lista de identificadores de hardware y de identificadores compatibles con dicho bus. Esta lista está ordenada desde el driver más específico al menos específico. En caso de encontrar ocurrencias en más de un fichero INF, el fichero donde la ocurrencia sea más específica tendrá preferencia. Si solo se encuentra un driver compatible, puede pedirse al usuario que provea al sistema de un driver más actual.

7.5.13. Gestor de consumo.
	El hardware que quiera utilizar el gestor de consumo debe cumplir la interfaz avanzada de configuración y consumo(Advanced Configuration and Power Interface, ACPI). Esta interfaz estándar define seis estados de consumo fundamentales, del S0(funcionamiento completo) al S5(apagado). Las características principales de estos estados son su consumo energético, el estado del que se realiza la transición a un estado más activo, y la latencia hardware.
	Del estado S1 al S3(inclusive) los estados existentes son los conocidos estados de bajo consumo(Sleeping States) en los que el consumo del dispositivo se reduce hasta casi parecer apagado, salvando la información relevante en RAM y en disco para retomar las tareas por donde se dejaron.
	El estado S4 es el de hibernación, en el que se almacena una imagen de la memoria RAM, comprimida, en el fichero Hiberfil.sys en el directorio raíz del sistema. Después de ello el sistema procede al apagado completo. Durante el inicio, el sistema comprueba si la imagen en dicho fichero es válida, y en caso afirmativo, el proceso Bootmgr ejecuta Winresume, que carga el contenido del fichero en la memoria, y restaura la ejecución.
	Es posible que si el hardware lo soporta, se pase a un estado híbrido entre el S3 y S4 llamado hybrid sleep, en el sistema pasa al estado S3 almacenando un fichero de hibernación en disco. Para hacer más rápida esta operación, el fichero almacenado solo guarda los datos que no pueden ser almacenados en disco posteriormente. Los drivers recibirán la notificación de transición al estado de hibernación S4. Si la energía se consume completamente, el sistema dispone de lo necesario para retomar las tareas en curso, en el fichero de hibernación.
	Si se desea pasar de un estado de bajo consumo a otro, se hace a través del estado S0, de operación completa.
	A pesar de existir estos seis estados de consumo, ACPI define 4 de ellos, siento el estado D0 el de operación completa, y el D3 el de apagado. Deben ser los drivers de dispositivos los que definan los estados D1 y D2, cumpliéndose que, en D2 debe consumirse menos que en D1. Microsoft, junto con los fabricantes de hardware definen unas especificaciones de referencia.
	El gestor de consumo, entonces, dialoga con los drivers de dispositivos para realizar su cometido. El gestor se encarga de definir las políticas adecuadas, decidiendo cual es el estado apropiado en un momento determinado, y solicitando a los drivers que realicen las transiciones requeridas. Los factores que se tienen en cuenta para definir que política es la adecuada  son el nivel de actividad, el nivel de batería, las peticiones de apagado, hibernación y bajo consumo(dormido) de las aplicaciones, las acciones de usuario(pulsar el botón de apagado, cerrar el portátil, etc.) y la configuración del panel de control de consumo.
	Al realizar la enumeración, parte de la información que se ofrece al gestor de Plug and Play son sus capacidades de consumo energético. De entre ellas se encuentra información acerca de si soporta los estados D1 y D2, sus latencias, o el tiempo requerido para las transiciones de estado. La correspondencia entre los estados que soporta el sistema y los ofrecidos por ACPI se resumen en la siguiente tabla:
	
System Power State
Device Power state
S0(operación completa)
D0(operación completa)
S1(dormido)
D1
S2(dormido)
D2
S3(dormido)
D2
S4(Hibernación)
D3(Apagado)
S5(Apagado completo)
D3(Apagado)

	Cuando el gestor de consumo decide realizar una transición de estados de consumo, envía el comando determinado a la rutina de resolución del driver de dispositivo. Para las tareas de consumo solo un driver se designa como encargado de satisfacer las políticas de consumo. El sistema preguntará a este drive su política a través de la función PoRequestPowerIrp, notificando al resto de drivers la acción a realizar por el la rutina de resolución de consumo. De esta forma, el gestor de consumo siempre sabe qué comandos están activos y en que momento.
	Aun así, los drivers pueden realizar transiciones de forma unilateral, permitiéndose realizar transiciones a estados de más bajo consumo en caso de que esté inactivo por un amplio periodo de tiempo(p.e. parar del motor de un disco duro). En el otro sentido, el driver también es capaz de utilizar los servicios proveídos por el gestor de consumo registrandose mediante la función PoRegisterDeviceForIdleDetection. Esta rutina informa de los valores de temporizadores que detectan que el dispositivo está ocioso, y su estado de consumo requerido cuando se encuentra en este estado. Para ello usa dos límites temporales principales, uno configurado por el sistema,  y el otro configurado por el usuario para un optimo aprovechamiento de los recursos. Para notificar que el dispositivo está activo utiliza la función PoSetDeviceBusy.
	En ningún caso el dispositivo puede provocar una transición de estado en el sistema, puesto que el gestor de consumo notifica la transición, pero nunca pregunta si el dispositivo está listo para hacerla.
	El espacio de usuario puede registrar notificaciones de estado de consumo al núcleo.

7.6 Subsistema de almacenamiento.
	El subsistema de almacenamiento se encarga de gestionar los dispositivos de bloques. Al tratarse del mismo tipo de dispositivo utilizado en GNU/Linux, muchas características se repiten, como puede ser la organización en bloques de 512 bytes(2048 bytes si se trata de un CD-ROM), el concepto de sectores, particiones, volúmenes(simples y multipartición), etc.
	Veremos, pues, como se gestionan dichos dispositivos en el sistema operativo de Microsoft.

7.6.1. Drivers de disco.
	Para este tipo de drivers se dispone de una pila de almacenamiento detallada en la siguiente figura:


	El primer componente que requiere el uso del disco es Winload, que, propiamente dicho no pertenece a la pila representada previamente, sin embargo, ya que requiere acceder ala información de disco para obtener los ficheros de carga del sistema operativo, está obligado a soportar el acceso a estos. Para ello Bootmgr ofrecerá las opciones de inicio al usuario para que decida desde que partición iniciar el sistema, para lanzar la ejecución de Winload. Este cargará en memoria los ficheros de sistema de Windows, el registro, el núcleo Ntoskrnl.exe, y sus dependencias. Para ello se nutre del firmware del sistema, que le ofrece lo mínimo indispensable para que pueda leer el contenido de disco.
	Para este tipo de drivers se ofrece una arquitectura basada en clases, puestos y minipuertos, donde los drivers de clases ofrecen funcionalidades comunes a todo tipo de dispositivo, los drivers de puerto ofrecen las funcionalidades comunes a un tipo de bus de dispositivos de almacenamiento, y el driver de minipuerto comunicará con un controlador particular de almacenamiento.
	Los drivers de clase conforman la interfaz estándar de dispositivos de Windows. Los drivers de minipuertos utilizan la interfaz de drivers de puertos, en vez de la interfaz de dispositivos, y los drivers de puertos implementan rutinas que ejerzan de interfaz entre los minipuertos y Windows. De esta forma, los desarrolladores de drivers de minipuerto pueden centrarse en la lógica específica del hardware. Las funcionalidades comunes a todo disco vienen implementadas en “\Windows\System32\Drivers\Disk.sys”. Otros drivers de interés son los ofrecidos por Scsiport.sys(para los buses SCSI), Ataport.sys(para los buses ATAPI) y Storport.sys(este último reemplaza Scsiport.sys con nuevas funcionalidades como la gestión de RAIDs y la comunicación por fibra óptica, entre otras.
	Los drivers de bus SCSI y ATAPI implementan un gestor de peticiones llamado C-LOOK en el que el driver inserta las peticiones en una lista ordenada por número de sector al que la petición va dirigida, utilizando las funciones KeInsertByKeyDeviceQueue y KeRemoveByKeyDeviceQueue para insertar y eliminar elementos respectivamente. El driver entonces recorrerá la lista de peticiones resolviéndolas hasta que alcance el final. Puesto que durante la resolución de peticiones, habrán aparecido nuevas peticiones que atender, el driver retornará al comienzo de la lista y volverá a recorrerla. Esto provocará que la cabeza de disco haga un recorrido desde los cilindros exteriores a los interiores continuamente, en caso de que las peticiones se repartan a lo largo de todo el disco. Si no existe una mayor organización en la atención de peticiones en los drivers de bus, es debido a que las peticiones se dirigen a vectores de almacenamiento, donde no queda claro donde se encuentra ni el comienzo, ni el final del disco. Diferentes drivers de controladores de discos se incluyen en la instalación inicial de Windows.

7.6.2. Drivers iSCSI.
	Especial mención requieren este tipo de drivers por dar soporte a soluciones de almacenamiento distribuido como las conocidas Storage Area Networks(SAN), o areas de red dedicadas al almacenamiento. Para ello integran el protocolo SCSI con el protocolo TCP/IP. Las SAN suelen utilizar fibra óptica para comunicar los distintos nodos, pero puede utilizarse  Gigabit Etherrnet para crear redes de almacenamiento más baratas y con buen rendimiento.
	El componente principal que gestiona este almacenamiento es el invocador de software iSCSI(iSCSI Software Initiator) de Microsoft, y sus componentes principales se listan a continuación:
Invocador: Compuesto del driver Storport comentado anteriormente y el driver de minipuerto iSCSI(Msiscsi.sys) implementa el driver necesario para ofrecer soluciones SAN a través de Ethernet.
Servicio del invocador: Implementado en “\Windows\System32\Iscsiexe.exe”, descubre gestiona la seguridad de los invocadores, además de los inicios de sesión y finalizaciones de comunicación con otros nodos.
Gestor de aplicaciones: Iscsicli.exe es un cliente en consola que permite las ćonexiones seguras a las soluciones iSCSI, además de panel de control requerido para estos dispositivos.

7.6.3. Drivers de entrada/salida a través de múltiples caminos.
	Servidores que requieren un alto grado de disponibilidad utilizan este tipo de soluciones. Se trata de dispositivos de almacenamiento que disponen de más de un camino para ser alcanzados, permitiendo que se produzcan fallos en las comunicaciones a través de algunos de sus caminos, sin necesidad de que impida que el almacenamiento sea alcanzado por otras vías. En caso de que el sistema operativo no ofreciera soporte a estas situaciones, un mismo disco sería considerado dos distintos por el hecho de poder ser accedido por diferentes caminos. Este soporte se apoya en drivers propios, o de terceros llamados módulos específicos de dispositivo(device-specific modules), que especifican los detalles de los múltiples caminos.
	El soporte de múltiples caminos es ofrecido por la pila de almacenamiento MPIO(Multiple Path Input/Oputput, on entrada/salida por múltiples caminos) y es el driver Disk.sys quien se responsabiliza del dispositivo, localizando el módulo específico para este. Además avisará al driver Mpio.sys, que representa el driver de bus para dispositivos de múltiples caminos, de la presencia del dispositivo. En esta situación tenemos un total de 3 pilas de dispositivos de almacenamiento, dos representando los caminos físicos, y una representando el dispositivo. Cuando una petición alcanza el disco, el módulo DSM se encargará de decidir por que camino dirige la respuesta.

7.6.4. Objetos de dispositivos de disco.
	El driver de clase para dispositivo de disco es encargado de crear los objetos que los representan. En el espacio de nombres serán representados con la nomenclatura “\Device\HarddiskX\DRX”, donde X representa un número en cuestión. Para mantener la compatibilidad con sistemas previos se crean enlaces simbólicos con el formato ofrecido por windows NT 4, que formateaba los nombres con la nomenclatura “\Device\HarddiskX\PartitionX” para referirse a “\Device\HarddiskX\DRX”. También se generan enlaces hacia el objeto que representa al dispositivo físico, teniendo enlaces del tipo “\GLOBAL??\PhysicalDriveX” apuntando a “\Device\HarddiskX\DRX”, que, como comentamos previamente, permitían exportar los dispositivos al espacio de usuario para que las aplicaciones los utilizaran bajo demanda.

7.6.5. Discos básicos y discos dinámicos.
	En las placas bases estándar, los servicios mínimos de entrada/salida son ofrecidos por una pequeña memoria FLASH donde se almacena un fichero binario llamado BIOS(Basic Input/Output System). En este tipo de hardware, el primer sector del disco principal contiene el registro maestro de inicio(Master Boot Record, o MBR). Cuando la BIOS se prepara para iniciar el sistema operativo, acude a este registro para conseguir la información necesaria acerca de los volúmenes existentes y de cual debe iniciar. Los volúmenes se registran en la tabla de particiones, dentro del registro maestro de inicio. En una tabla de particiones de este tipo pueden definirse hasta cuatro particiones principales, con sus correspondientes tipos(usualmente FAT32 y NTFS). 
	Pueden añadirse más particiones definiendo particiones extendidas, que contienen su propio registro maestro de inicio, y son llamados dispositivos lógicos. Teóricamente, existe un límite de particiones extendidas de cuatro por disco, sin embargo, Windows es capaz de controlar particiones extendidas anidadas, permitiendo que ese límite desaparezca.
	Estas características conforman la estructura de un disco básico.
	Hoy en día existe una iniciativa propuesta por Intel para promover la estandarización desu plataforma de firmware extendido(Extended Firmware, o EFI). Esta plataforma conforma un sistema operativo mínimo almacenado en memoria FLASH, que sirve para cargar herramientas básicas de diagnóstico, y la carga del sistema operativo. Este tipo de firmware define un nuevo tipo de tabla de particiones basado en identificadores únicos globales(Global Unique Identifiers, o GUIDs), llamado tabla de particiones basado en GUID(GUID Partition Table, o GPT). El objetivo principal de dicho tipo de tabla de particiones es eliminar las restricciones(sobre todo a la hora de redimensionar volúmenes) de las antiguas tablas de particiones basadas en registros maestros de inicio. Este tipo de tablas de particiones conforman lo que se conocen como discos dinámicos.

7.6.6. El espacio de nombres de volúmenes.
	Este es el mecanismo de Windows que se encarga de asignar letras a los objetos de los dispositivos de bloques, permitiendo un acceso familiar a las aplicaciones, y ofreciendo la posibilidad de montar y desmontar los volúmenes.

7.6.6.1. Gestor de montaje.
	El driver del gestor de montaje(Mountmgr.sys) es el encargado de asignar las letras a los volúmenes creados después de la instalación de Windows. Las asignaciones se almacenan en “\HKLM\SYSTEM\MountedDevices”, y su clave tiene la forma “\??\Volume{X}” donde X es el GUID del volumen. Los valores asociados tienen la forma “\DosDevices\X:\” siendo X la letra asignada a la unidad. Todo volumen debe tener una entrada en el espacio de nombres, pero no tiene por qué tener asignada una letra de unidad.
	Para los discos básicos, el registro almacena las letras de sus volúmenes, los nombres en estilo Windows NT 4, y la dirección de comienzo del volumen asociado. En el caso de discos dinámicos, solo se almacena el GUID. Durante el inicio, el gestor de montaje se registra en el sistema de Plug and Play, de forma que cuando un dispositivo es identificado como un volumen, recibe la notificación pertinente del sistema de PnP. Con esta notificación, el gestor de volúmenes determina el GUID para localizar la letra de la unidad almacenada en el registro de Windows. De no encontrarla, sugerirá una y la almacenará en el registro. En el caso de los discos básicos, no se devuelve sugerencia. De no existir esta, se utilizará la primera letra no asignada a otro volumen, se define su asignación, se crean los enlaces simbólicos necesarios y finalmente se actualiza el registro de Windows. Si se diera el caso de que todas las letras estuvieran utilizadas, no se ofrecería ninguna asignación.
	Es necesario resaltar que el GUID utilizado por el gestor de volúmenes no necesariamente debe coincidir con el GUID real del volumen, permitiendose así que los discos básicos dispongan de un GUID que su tabla de particiones no le asigna inicialmente.

7.6.6.2. Puntos de montaje.
	Los puntos de montajes permiten enlazar volúmenes en directorios, dentro de los volúmenes NTFS. Este es el mecanismo ofrecido por Windows, para manejar los volúmenes que no tuvieron la suerte de disponer de una letra asignada. Para ello utiliza una tecnología de reprocesado de puntos de montaje.
	El punto reprocesado, es un bloque de datos arbitrario, con una cabecera fija que Windows asocia a un fichero o directorio de un volumen NTFS. Puede ser una aplicación, o el mismo sistema, quien defina el formato y comportamiento de estos puntos. Dentro de la cabecera se almacenará una etiqueta que defina si fue definida por una aplicación, o por el sistema, el tamaño, y el significado del punto reprocesado. De ser una aplicación la que lo define, debe proveer de un driver de filtro para revisar los valores devueltos por el volumen al operar con sus ficheros. Estos valores son devueltos en caso de encontrar una coincidencia de fichero o directorio en la operación realizada.
	Este sistema se implementa haciendo uso del sistema de ficheros NTFS, el gestor de entrada/salida, y el gestor de objetos. Este último inicia el procesado de localizaciones, sirviendose del gestor de entrada/salida como interfaz con el sistema de ficheros. Sin embargo, el gestor de objetos debe volver a realizar las operaciones para las que el gestor de entrada/salida devuelve un código de estado. El gestor de entrada/salida implementa la modificación de la localización y los posibles puntos reprocesados necesarios para que el sistema de ficheros pueda finalizar la operación. Puede entenderse que el gestor de entrada/salida funciona como un driver de filtro para el sistema de reprocesado de puntos de montaje.
	El uso más utilizado de esta tecnología, es la creación de enlaces simbólicos en NTFS.  Si el gestor de entrada/salida recibe un código de estado del sistema de reprocesado, y el fichero o directorio no está asociado con un punto de montaje reprocesado, ningún driver de filtro se har´a cargo de la operación, devolviendo un código de error al gestor de objetos.
	Los puntos de montajes son, pues, puntos reprocesados que almacenan un nombre de volumen. Es posible utilizar el gestor de discos para crear puntos de montaje.
	El gestor de montaje alberga una base de datos en la que almacenará los puntos de montajes definidos para cada volumen. Esta se aloja en el directorio “System Volume Information” de cualquier volumen NTFS.

7.6.6.3. Montaje de volúmenes.
	Aunque un volumen disponga de letra correspondiente, el contenido del volumen puede estar organizado por un sistema de ficheros no soportado por Windows. Para reconocer el volumen, un sistema de ficheros debe proclamar que la partición le pertenece, proceso que sucederá la primera vez que el núcleo, un driver, o una aplicación accedan a un fichero del volumen. Una vez proclamada la pertenencia, el gestor de entrada/salida redirige todos los paquetes de peticiones al driver correspondiente. El montaje implica a tres componentes: el registro del driver de sistema de ficheros, los bloques de parámetros de volumen(Volume Parameters blocks, o VPBs) y las peticiones de montajes.
	El gestor de entrada/salida supervisa el proceso de montaje, y es consciente de que existen los drivers de sistemas de ficheros puesto que son registrados en la inicialización de estos mismos, a través de la función IoRegisterFileSystem. Al registrarse, una referencia será almacenada en una lista que el gestor de entrada/salida utiliza durante las operaciones de montaje.
	Los bloques de parámetros de volúmenes se utilizan como enlaces entre el objeto que representa un volumen, y el objeto de dispositivo. Si su referencia está vacía, ningún sistema de ficheros habrá montado el volumen, y el gestor de entrada/salida deberá comprobar la integridad de la referencia si se utiliza la operación open sobre un fichero del volumen.
	La convención utilizada por los sistemas de ficheros para comprobar si el volumen fue montado con su formato, es examinar el registro de inicio del volumen, almacenado al comienzo de este. En los sistemas de ficheros de Microsoft, este registro contiene un campo que almacena el sistema de ficheros utilizado, aparte de información necesaria para que el driver de sistema de ficheros encuentre ficheros de metadatos críticos en el volumen.
	Si el sistema de ficheros afirma que el volumen le pertenece, el gestor de entrada/salida rellena el bloque de parámetros de volumen y pasa la petición de apertura con el resto de la localización al driver del sistema de ficheros. Este finalizará la petición utilizando su formato para interpretar los datos que el volumen contiene. Una vez el montaje rellena el bloque de parámetros, el gestor de entrada/salida puede resolver el resto de aperturas dirigidas al volumen montado. Si ningún sistema de ficheros proclama la pertenencia del volumen, el sistema de ficheros en bruto(Raw) lo hará, y toda petición de apertura de fichero fallará.
	Para evitar tener todos los drivers de sistemas de ficheros cargados, aunque no dispongan de volúmenes que gestionar, Windows intenta minimizar el uso de memoria delegando el reconocimiento del sistema de ficheros en un driver llamado reconocedor de sistema de ficheros(File System Recognizer). Este conoce lo estrictamente necesario para reconocer si el sistema de ficheros estudiado se asocia a uno de los soportados por Windows. Durante el inicio, el reconocedor es registrado como un driver de sistema de ficheros, y es llamado por el gestor de entrada/salida cada vez que se solicita una operación de montaje. Es entonces cuando el reconocedor cargará el driver del sistema de fichero en memoria y redirigirá las peticiones a este, dejando que el sistema de ficheros proclame la pertenencia del volumen.
	Cuando se solicita una comprobación de consistencia de sistema de ficheros a través de la aplicación Chkdsk, durante el comienzo del inicio, casi todos los volúmenes son montados. La aplicación que ejerce dicha comprobación es Autochk.exe, y es invocada por el gestor de sesiones(Smss.exe) cuando es especificada para ejecutarse en el inicio a través de la clave de registro “HKLM\SYSTEM\CurrentControlSet\Control\Session Manager\BootExecute”. Chkdsk accede a todos lis dispositivos para ver si requieren la comprobación de consistencia.
	El montaje de un mismo dispositivo puede suceder en varias ocasiones cuando se hace referencia a dispositivos desmontables. Si se vuelve a montar un dispositivo previamente montado se le pregunta por su identificador de volumen, si este ha cambiado, lo desmontará para remontarlo posteriormente.

7.6.6.4. Operaciones de entrada/salida en volúmenes.
	Cuando el sistema de ficheros recibe una petición de entrada/salida, delega en el gestor de volúmenes para transferir datos desde y hacia el disco donde reside el volumen. En el proceso de montaje, el sistema de fichero recibe la referencia del objeto de volumen con el que enviará las peticiones al gestor de volúmenes. Sin embargo, si una aplicación lo requiere, puede obviar el sistema de ficheros y acudir directamente al objeto de volumen para cumplir sus labores(p. e. programas de restauración de ficheros eliminados).
	Cuando una petición se envía al objeto de volumen, el gestor de entrada/salida lo redirige al gestor de volúmenes que creó el objeto de dispositivo. El gestor de volúmenes debe entonces traducir el desplazamiento para localizar el fichero dentro del volumen, a un desplazamiento dentro del disco. Este desplazamiento será enviado al driver de clase de disco, que a su vez, delegará en el driver de minipuerto. Este último se encarga de resolver la petición de entrada/salida finalizando la operación. El panorama se resume en la siguiente figura:
	

7.6.7. Servicio de discos virtuales.
	Los desarrolladores de productos de almacenamiento, facilitan una serie de herramientas para gestionar sus dispositivos, que, como cabe esperar, suponen una dificultad a salvar por los administradores de sistemas. En ocasiones, es necesario que aprendan múltiples interfaces para desarrollar aplicaciones específicas debido a que las herramientas de Windows no son capaces de resolver las peticiones.
	Para facilitar este tipo de labores, el sistema ofrece el servicio de discos virtuales, que supone una capa de abstracción que unifica el almacenamiento a alto nivel. De esta forma, el administrador puede utilizar los distintos dispositivos, sirviéndose de una interfaz estándar.
	Las interfaces ofrecidas por este subsistema son dos, una para proveedores de software, y otra para proveedores de hardware:

La interfaz para proveedores de software supone una interfaz con el alto nivel, y trabaja con discos, particiones y volúmenes. Ofrece operaciones como la creación, redimensionado, y eliminación de volúmenes, además de crear y destruir unidades réplicas de otras unidades, y crear y formatear las unidades. Las aplicaciones deben registrarse en la clave “HKLM\SYSTEM\CurrentControlSet\Services\Vds\SoftwareProviders”. Se ofrecen interfaces para trabajar con unidades dinámicas o básicas.
Los proveedores de hardware deben implementar su software como librerías dinámicas(DLLs) y registrarlas en la clave “HKLM\SYSTEM\CurrentControlSet\Services\Vds\HardwareProviders”. Dicha librería se encargará de traducir los comandos independientes del almacenamiento proveídos por VDS, a comandos específicos del almacenamiento que ofrecen.

	Si una aplicación requiere la API de VDS, y el servicio no se encuentra en funcionamiento, Svchost recibirá la petición e iniciará el servicio(Vdsldr.exe), el cual, gestionará las peticiónes requeridas y, en la finalización de comunicaciones con la API, terminará su ejecución.

7.7 Subsistema de redes. 


8. Glosario.

Cloud Storage:  almacenamiento en la nube, es decir, en internet.
Agilidad:  capacidad de mover los datos al área adecuada para mejorar su disponibilidad.
Escalabilidad: capacidad de manejar crecientes cargas de trabajo, o de adaptarse para ser capaces de soportarlas.
Elasticidad: capacidad de escalarse mas allá limites impuestos por las condiciones del sistema.
Latencia: Tiempo de espera, normalmente entre que se realiza una petición y se recibe una respuesta.
Persistencia: Si existen distintas copias de un archivo, todas deben ser iguales.
Cluster: Conjunto de sistemas informáticos que actúan como un solo sistema.
Nodo: Sistema capaz de procesar información.
Multiusuario: capacidad de que el recurso sea utilizado por varios usuarios concurrentemente.
Servicio distribuido: servicio ofrecido por un conjunto de sistemas que actúa como un solo sistema.
API: Application programming interface, interfaz de programación de la aplicación.
Tolerancia a fallos: capacidad de soportar errores sin dejar sin servicio al cliente.
Redundancia de datos: duplicación de datos, habitualmente para evitar su pérdida.
Software duradero: software capaz de almacenar distintas versiones de un mismo fichero.
World Wide Web: Sistema de enlace de documentos a través de internet.
Amazon EC2 Query Interface: Interfaz de consultas de la plataforma de Cloud Computing de Amazon.
Amazon S3: Sistema de almacenamiento propietario de la plataforma de Cloud Computing de Amazon.
Data store drivers: Drivers de almacenamiento de datos.
Transfer manager drivers: Drivers de gestión de transferencias.
Imagen virtual: fichero que simula el disco duro de un ordenador para, en conjunto con la tecnología de virtualización, se pueda lanzar un sistema operativo con sus aplicaciones propias.
Sector: Unidad mínima de almacenamiento de un dispositivo de bloques.
Buffer: zona de memoria para alojar información.
Cache: memoria pequeña pero muy rápida, dentro de la jerarquía de memorias.
Puntero: tipo abstracto de dato útil para referenciar direcciones de memoria.
Memoria principal: almacenamiento de instrucciones y datos principal del sistema físico, habitualmente, la memoria RAM.
Partición: Zona de almacenamiento alojada dentro de un dispositivo de bloques.
Volumen: Objetos que representan sectores de disco.
Volumen simple: Objeto que representa sectores de una sola partición.
Volumen multipartición: Objeto que representa sectores de múltiples particiones como uno solo.
Enlace simbólico: enlace entre un punto determinado del dispositivo físico y un fichero alojado en otro punto completamente distinto.
Metadatos: Información adicional de un fichero o directorio, necesario para su adecuada gestión por los distintos subsistemas del sistema operativo.
Espacio de nombres: juego de identificadores que permiten la desambiguación de homónimos.
IRQ: Interrupt request, o número de interrupción del dispositivo para detener la ejecución de código en la CPU.
DMA: Direct Memory Access, metodo de acceso a memoria en el que se delega la tarea del acceso a la información a un dispositivo intermedio que permita al procesador continuar con sus labores sin apenas provocandole mínimos retrasos.
Socket: Abstracción utilizada en los sistemas operativos Unix para representar una conexión de red.
Spinlock: Mecanismo de espera activa provocada cuando se requiere adquirir un recurso no disponible.
Callback: Función ejecutada, tan pronto como es posible, para atender la situación registrada por un nuevo evento.
Mutex: Mecanismo de concurrencia basado en una variable, que recibe, normalmente, dos valores, abierto y cerrado, y en función de el, detiene la ejecución de código hasta que su valor cambie, reanudandola.
Jiffies: Unidad de tiempo mínima del núcleo de Linux.
Dispositivos Plug and Play: dispositivos que, al conectarse al sistema, se configuran automáticamente y se ofrecen sus servicios al usuario sin que este realice ninguna tarea previa.
	
	Referencias.

Wikipedia:
http://en.wikipedia.org/wiki/Cloud_computing_comparison
http://en.wikipedia.org/wiki/OpenNebula
http://en.wikipedia.org/wiki/Nimbus_(cloud_computing)
http://en.wikipedia.org/wiki/Amazon_Elastic_Block_Store
http://en.wikipedia.org/wiki/Eucalyptus_%28computing%29
http://en.wikipedia.org/wiki/Cloudstack 
http://en.wikipedia.org/wiki/OpenQRM
http://en.wikipedia.org/wiki/Abiquo_Enterprise_Edition 
http://en.wikipedia.org/wiki/OpenStack
Amazon:
http://aws.amazon.com/s3/
OpenNebula:
http://opennebula.org/documentation:rel3.6
Nimbus:
http://www.nimbusproject.org/docs/2.9/ 
CloudStack:
http://docs.cloudstack.org/CloudStack_Documentation 
OpenQRM:
http://www.openqrm-enterprise.com/news/details/article/in-depth-documentation-of-openqrm-available.html 
Abiquo:
http://community.abiquo.com/display/ABI20/Abiquo+Documentation+Home
OpenStack:
http://www.openstack.org/software/openstack-storage/
http://docs.openstack.org/api/
http://docs.openstack.org/developer/
http://docs.openstack.org/developer/swift/development_saio.html
Linux Kernel Development – Robert Love, Editorial Addison Wesley.
Linux Device Drivers – Jonathan Corbet, Alessandro Rubini and Greg KroaH-Hartman, Editorial O'Reilly.
Understanding the linux kernel – Daniel P. Bovet and Marco Cesati.
Windows Internals – Mark E. Russinovich and David A. Solomon.

